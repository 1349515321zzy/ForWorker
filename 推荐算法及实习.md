# 实习项目

### 1.Transformer问题

https://jishuin.proginn.com/p/763bfbd565fc
#### 1.1. attention里面softmax之前要除以根号D，为什么

缓解梯度消失问题。作者在论文中的解释是点积后的结果大小是跟维度成正比的，所以经过softmax以后，梯度就会变很小，除以 dk 后可以让 attention 的权重分布方差为 1，否则会由于某个输入太大的话就会使得权重太接近于1（softmax 正向接近 1 的部分），梯度很小，造成参数更新困难。
#### 1.2position embedding中为什么要采用加和，而不是三个embedding concat后再做变换，这两者区别是什么
    Embedding的数学本质，就是以one hot为输入的单层全连接。现在我们将token,position,segment三者都用one hot表示，然后concat起来，然后才去过一个单层全连接，等价的效果就是三个Embedding相加。
#### 1.3 bert预训练任务

bert是用了transformer的encoder侧的网络，作为一个文本编码器，使用大规模数据进行预训练，预训练使用两个loss，一个是mask LM，遮蔽掉源端的一些字（可能会被问到mask的具体做法，15%概率mask词，这其中80%用[mask]替换，10%随机替换一个其他字，10%不替换），然后根据上下文去预测这些字，一个是next sentence，判断两个句子是否在文章中互为上下句，然后使用了大规模的语料去预训练

#### 1.4为什么要使用多头
可以类比CNN中同时使用多个滤波器的作用，直观上讲，多头的注意力有助于网络捕捉到更丰富的特征/信息。也可以把多头注意力看作是一种ensemble，模型内部的集成。。使用多头注意力，也就是综合利用各方面的信息/特征。可以注意到不同子空间的信息，捕捉到更加丰富的特征信息。
#### 1.5 Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？
使用Q/K/V不相同可以保证在不同空间进行投影，增强了表达能力，提高了泛化能力。
我们知道K和Q的点乘是为了得到一个attention score 矩阵，用来对V进行提纯。K和Q使用了不同的W_k, W_Q来计算，可以理解为是在不同空间上的投影。正因为有了这种不同空间的投影，增加了表达能力，这样计算得到的attention score矩阵的泛化能力更高
请求和键值初始为不同的权重是为了解决可能输入句长与输出句长不一致的问题。并且假如QK维度一致，如果不用Q，直接拿K和K点乘的话，你会发现attention score 矩阵是一个对称矩阵。因为是同样一个矩阵，都投影到了同样一个空间，所以泛化能力很差。
#### 1.6为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？
答：多头注意力层和激活函数层之间。CV使用BN是认为channel维度的信息对cv方面有重要意义，如果对channel维度也归一化会造成不同通道信息一定的损失。而同理nlp领域认为句子长度不一致，并且各个batch的信息没什么关系，因此只考虑句子内信息的归一化，也就是LN。

#### 1.7简答讲一下BatchNorm技术，以及它的优缺点。
答：批归一化是对每一批的数据在进入激活函数前进行归一化，可以提高收敛速度，防止过拟合，防止梯度消失，增加网络对数据的敏感度。

#### 1.8Transformer的并行化提现在哪个地方？
答：Transformer的并行化主要体现在self-attention模块，在Encoder端Transformer可以并行处理整个序列，并得到整个输入序列经过Encoder端的输出，但是rnn只能从前到后的执行
#### 1.9 为什么transformer没用position embedding 而是position encoding？
两种方法都能取得一样的效果，其中一种方法明显的直观简单，所有大家自然的选择简单的方法.BERT 虽然使用了 Transformer 的结构，但是对于细节策略的选择，模型的学习目标比模型结构更重要。

#### 1.4高维稀疏特征场景中 LR 比 GBDT 效果好的原因？
对于高维稀疏数据，线型模型往往结合 L1/L2 正则项，对于部分过拟合特征的惩罚会随着该部分特征值的增大而相应提高；(带正则项的线性模型不容易对稀疏特征过拟合)
而对于树模型，往往只需要部分过拟合特征做分裂节点就可以完美分割样本，其很难通过限制树深度或者叶子节点数等参数达到抑制该部分过拟合的特征。
![image](https://user-images.githubusercontent.com/49945008/128655221-c3185da9-69d0-43a8-8a37-adc2c43c6749.png)

### 2.胶囊网络

​	阿里的基于动态路由的用户多兴趣网络MIND，基于胶囊路径机制的多兴趣提取层，适用于聚类历史行为，提取不同的兴趣，获取多个向量表达用户兴趣的不同方面。

​	在推荐过程最重要的一环就是如何对用户不同阶段的兴趣进行有效表达。大多数目前的深度网络模型中，都是构建一个用户和Item 统一的向量空间中去获取用户的兴趣表达。如 DIN 模型 (Deep Interest Network)：通过挖掘用户的历史行为，利用Attention 机制捕获用户兴趣分布，表达用户多种多样的兴趣爱好。这类模型都是将用户映射为单个向量以表达用户兴趣，通常不足以捕获用户不同阶段、不同性质的兴趣分布。MIND) 是通过构建用户和商品向量在统一的向量空间的多个用户兴趣向量，以表达用户多样的兴趣分布。然后通过向量召回技术，利用这多个兴趣向量去检索出TopK个与其近邻的商品向量，得到 TopK个 用户感兴趣的商品。

​		如果把与用户兴趣各种相关的信息都压缩成为一个表达向量，这会成为用户多样兴趣表达的瓶颈，在推荐召回阶段召回候选集时，对用户不同兴趣的所有信息混合在一起使用，会导致召回Item 的相关性大大降低。因此，我们采用多个向量来表达用户不同的兴趣，将用户的历史行为分组到多个 interest capsules 的过程, 期望属于同一个capsules 的相关Item 共同表达用户兴趣的一个特定方面。

​		假设我们有两层Capsule，我们从第一层和第二层引用 Capsule分别为低级Capsule和高级Capsule。动态路由(Dynamic Routing) 的目标是在迭代中给出低级别Capsule的值来计算高级别Capsule的值方法。

​		通过多兴趣提取器层，对用户行为序列embedding 我们得到多个个兴趣Capsule 表达用户多样的兴趣分布，不同的兴趣Capsule表示用户兴趣的不同偏好。为了评估多个兴趣Capsule对目标Item 相关度及贡献度，我们设计标签意识的Attention 机制来衡量目标Item 选择使用哪个兴趣Capsule：

简单来讲，分为三个步骤：首先对输入向量利用矩阵W相乘，得到浅层胶囊 [公式] ，然后对其进行加权相加得到s，最后利用squash函数非线性变化，得到深层胶囊向量 [公式] 。模型中有两个参数，W与b，其中W利用反向传播算法进行更新，b利用 [公式] 进行更新，算法流程如下图所示。
https://pic4.zhimg.com/v2-0f2577af29989718a183f555edcb369f_r.jpg
至此已经叙述了胶囊网络的简单算法流程，下面简单谈一谈对各个步骤的直观理解，可以忽略不计。

### 3.采样概率修正算法。

​	YouTube采样修正的双塔模型论文。

解决的问题：召回看做一个多分类问题，多分类问题中最常使用的激活函数就是Softmax，当类别数量特别大时，使用Softmax来训练模型是比较耗时的。所以一种常用的方法就是进行采样，工业界目前采用流式训练双塔结构一般是通过随机mini-batch内负采样的方式来优化损失函数。这种训练方式存在的一个显著问题就是in-batch loss会因为随机采样偏差而导致模型效果不好，尤其是当样本分布出现明显倾斜的时候。这种**batch内负采样的方法具有采样偏差，**无法保证采样后的分布和原始数据分布是一样的，这样模型学习到的将会是错误的分布从而影响模型的效果。

​		对采样概率进行自适应修正的算法，可以从流式数据中预估item的频率。

​		流式训练中，batch内随机负采样效果不佳，如何能拿到item的出现频次，做Negative Sampling呢？我们知道流式训练，数据是以滚动的方式输入到网络模型中的，我们无法维护一个动态且占高内存的词表及item频次信息。

​		进行batch内负采样的时候，item被采作负样本的概率约等于其出现在原始样本中的概率。这样会导致一些热门的受欢迎的item更容易被当成负样本。而在大多数推荐场景中有很明显的热点效应，对于这些热门item的过度打压会使得模型倾向于推荐一些冷门的item，从而影响线上表现。

解决方法：

​	采用流式训练，因此不断会有新物品出现，那么使用固定长度的词表不太合适，因此采用hash的方式来对物品的采样概率 ![[公式]](https://www.zhihu.com/equation?tex=p_%7Bj%7D)进行更新。

​	具体来说，首先定义一个hash函数 ![[公式]](https://www.zhihu.com/equation?tex=h%28y%29) ，把所有视频ID都映射到 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cleft%5B+0%2C+H+%5Cright%29+) 之间。这里hash一下是因为视频是动态的，随时都可能有新的视频进入系统，所以用hash函数映射一下固定住视频库大小。同时使用两个长度为H的数组 ![[公式]](https://www.zhihu.com/equation?tex=A) 和 ![[公式]](https://www.zhihu.com/equation?tex=B) ，通过 ![[公式]](https://www.zhihu.com/equation?tex=h%28y%29) 来得到给定的 item ![[公式]](https://www.zhihu.com/equation?tex=y) 在数组A和B中下标。

- ![[公式]](https://www.zhihu.com/equation?tex=A%5Bh%28y%29%5D) : 表示 ![[公式]](https://www.zhihu.com/equation?tex=item) ![[公式]](https://www.zhihu.com/equation?tex=y) 上一次被采样的step；
- ![[公式]](https://www.zhihu.com/equation?tex=B%5Bh%28y%29%5D) ：表示 ![[公式]](https://www.zhihu.com/equation?tex=item) ![[公式]](https://www.zhihu.com/equation?tex=y) 平均多少个step被采样一次，即采样频率。这里频率的意思是预估每过多少步可以被采样到一次，那么倒数就是预估被采样到的概率。

当第 ![[公式]](https://www.zhihu.com/equation?tex=t) 步物品 ![[公式]](https://www.zhihu.com/equation?tex=y) 被采样到，基于如下的公式更新 ![[公式]](https://www.zhihu.com/equation?tex=B%5Bh%28y%29%5D) ：

![[公式]](https://www.zhihu.com/equation?tex=B%5Bh%28y%29%5D+%5Cleftarrow+%281-%5Calpha%29+%5Ccdot+B%5Bh%28y%29%5D+%2B+%5Calpha+%5Ccdot+%28t-+A%5Bh%28y%29%5D+%29%5C%5C)

其中， ![[公式]](https://www.zhihu.com/equation?tex=A%5Bh%28y%29%5D) 则被赋予当前的训练步数 ![[公式]](https://www.zhihu.com/equation?tex=t) 。当训练完成时，预估的物品 ![[公式]](https://www.zhihu.com/equation?tex=y) 的采样概率是 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BB%5Bh%28y%29%5D%7D) 。从数组 ![[公式]](https://www.zhihu.com/equation?tex=B) 的定义可以看出， ![[公式]](https://www.zhihu.com/equation?tex=B%5Bh%28y%29%5D) 越小，即在训练样本中采样到的概率越高，比如2，每两次就会被采样到一次，那么 ![[公式]](https://www.zhihu.com/equation?tex=p_%7Bj%7D) 会比较大，就代表比较热门。接着就是更新![[公式]](https://www.zhihu.com/equation?tex=B%5Bh%28y%29%5D)，其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) 可看作学习率，剩下的就是带入到损失函数 ![[公式]](https://www.zhihu.com/equation?tex=L_%7BB%7D%28%CE%B8%29) 中梯度下降求解的过程了。

​	这里还有一个问题，既然是hash过程，当 ![[公式]](https://www.zhihu.com/equation?tex=H%3CM) 时，就会存在冲突的情况。冲突的情况会导致 ![[公式]](https://www.zhihu.com/equation?tex=B%5Bh%28y%29%5D) 较小，因为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%28+t-A%5Bh%28y%29%5D+%5Cright%29) 会较小，从而导致采样概率预估过高。这里的改进方案是使用Multiple Hashings。即使用多组hash方程和数组A、B。当训练完成后，在线推理时，使用最大的一个B[h(y)]去计算采样概率。

### 4.Informer

问题：

Self-Attention 的二次计算：Self-Attention Mechanism 的点积操作使各层的时间复杂度和内存使用量均为 ![[公式]](https://www.zhihu.com/equation?tex=+O%28L%5E2%29) ;

**（Ⅱ）**作用于长输入的堆叠层存在内存瓶颈：J 个 encoder/decoder layer 的堆叠使得总内存使用量为 ![[公式]](https://www.zhihu.com/equation?tex=O%28J%C2%B7L%5E2%29) ，这限制了模型在接收长输入时的可伸缩性；



贡献：

**2）**提出了 ProbSparse Self-Attention Mechanism 并有效取代了常规的 Self-Attention Mechanism ，取得了 ![[公式]](https://www.zhihu.com/equation?tex=O%28LlogL%29) 的时间复杂度和 ![[公式]](https://www.zhihu.com/equation?tex=O%28LlogL%29) 的内存使用量；【**降低了常规 Self-Attention 计算复杂度和空间复杂度**】

**（3）**提出了在 J-Stacking Layers 中控制 attention scores 的 Self-Attention Distilling 操作权限，并将总空间复杂度降低到 ![[公式]](https://www.zhihu.com/equation?tex=O%28%282-%5Cepsilon%29LlogL%29) ;【**使用自注意蒸馏技术缩短每一层的输入序列长度，降低了 J 个堆叠层的内存使用量**】



解决方法：

有研究工作表明 Self-Attention Probability 的分布具有稀疏性，且在不显著影响性能的情况下，设计了一些对所有 ![[公式]](https://www.zhihu.com/equation?tex=p%28%7B%5Crm+k%7D_j%7C%7B%5Crm+q%7D_i%29) 都有选择性的计数策略。然而，这些工作局限于理论分析，遵循启发式的方法，用相同的策略解决每个多头自注意问题，限制了进一步的改进。

事实上，只有少数几个点积对贡献了大部分注意力，其他的可以忽略不计，即“稀疏性”自注意分数形成了一个长尾分布（幂律分布）

第 ![[公式]](https://www.zhihu.com/equation?tex=i+) 个 query 对所有 key 的 attention 定义为概率 ![[公式]](https://www.zhihu.com/equation?tex=p%28%7B%5Crm+k%7D_j%7C%7B%5Crm+q%7D_i%29) ，输出是其与 values ![[公式]](https://www.zhihu.com/equation?tex=%5Crm+v) 的组合。主要的点积对促使相应 query 的 attention 概率分布远离均匀分布，即序列中的某个元素只会和少数几个元素具有较高的关联性。如果 ![[公式]](https://www.zhihu.com/equation?tex=p%28%7B%5Crm+k%7D_j%7C%7B%5Crm+q%7D_i%29) 接近于均匀分布 ![[公式]](https://www.zhihu.com/equation?tex=q%28%7B%5Crm+k%7D_j%7C%7B%5Crm+q%7D_i%29%3D%5Cfrac%7B1%7D%7BL_k%7D) ，那么 self-attention 就成了 values ![[公式]](https://www.zhihu.com/equation?tex=%5Crm+v+) 的平凡和，且对于输入而言是多余的。也就是说，每个概率值都趋近于 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BL_k+%7D) ，这种均匀的概率值与 query 结合并没有什么意义。反之，如果一个 query 得到的概率值与均匀分布相差很大，则必然存在少数几个较大的概率值，显然在 Self-Attention 中，这些与大概率对应的 query 才是有实际意义的。那么，如何能得到这种长尾分布呢？

分布 ![[公式]](https://www.zhihu.com/equation?tex=p) 和 ![[公式]](https://www.zhihu.com/equation?tex=q) 之间的相似性可以用来区分重要的 query ，通过 KL-散度可以衡量这种相似性

ProbSparse Self-Attention 工作流程（引自 **[文章](https://zhuanlan.zhihu.com/p/355133560)** ）

（1）为每个 query 都随机采样部分的 key，默认值为 ![[公式]](https://www.zhihu.com/equation?tex=5%C2%B7%7B%5Crm+ln%7DL) ；

（2）计算每个 query 的稀疏性得分 ![[公式]](https://www.zhihu.com/equation?tex=M%28%7B%5Crm+q%7D_i%2C%7B%5Crm+K%7D%29) ；

（3）选择稀疏性得分最高的 N 个 query ，N 默认值为 ![[公式]](https://www.zhihu.com/equation?tex=5%C2%B7%7B%5Crm+ln%7DL) ；

（4）只计算 N 个 query 和 key 的点积结果，进而得到 attention 结果；

（5）其余的 L-N 个 query 就不计算了，直接将 Self-Attention 层的输入取均值（mean(V)）作为输出，这样可保证每个 ProbSparse Self-Attention 层的输入和输出序列长度都是 ![[公式]](https://www.zhihu.com/equation?tex=L) 。

整体时间复杂度为 ![[公式]](https://www.zhihu.com/equation?tex=O%28L%C2%B7%7B%5Crm+ln%7DL%29)



# 推荐算法

### 1.cbow 与 skip-gram的比较

​		cbow和skip-gram都是在word2vec中用于将文本进行向量表示的实现方法。在cbow方法中，是用周围词预测中心词，使用GradientDesent方法，不断的去调整周围词的向量。当训练完成之后，每个词都会作为中心词，把周围词的词向量进行了调整，这样也就获得了整个文本里面所有词的词向量。

​		而skip-gram是用中心词来预测周围的词。在skip-gram中，会利用周围的词的预测结果情况，使用GradientDecent来不断的调整中心词的词向量，最终所有的文本遍历完毕之后，也就得到了文本所有词的词向量。

​		skip-gram进行预测的次数是要多于cbow的：因为每个词在作为中心词时，都要使用周围词进行预测一次。这样相当于比cbow的方法多进行了K次（假设K为窗口大小），因此时间的复杂度为O(KV)，训练时间要比cbow要长。

### 2.wide & deep 模型 wide 部分和 deep 部分分别侧重学习什么信息

​		**Wide部分有利于增强模型的“记忆能力”，Deep部分有利于增强模型的“泛化能力”**

​		Memorization 通过一系列**人工的特征叉乘（cross-product）**来构造这些非线性特征，捕捉sparse特征之间的高阶相关性，即“记忆” 历史数据中曾共同出现过的特征对。

​		Generalization 为sparse特征学习低维的dense embeddings 来捕获特征相关性，学习到的embeddings 本身带有一定的语义信息。可以联想到NLP中的词向量，不同词的词向量有相关性，因此	Generalization是基于相关性之间的传递。这类模型的代表是DNN和FM。Generalization的优点是*更少的人工参与*，*对历史上没有出现的特征组合有更好的泛化性*

​		Memorization根据历史行为数据，产生的推荐通常和用户已有行为的物品直接相关的物品。而Generalization会学习新的特征组合，提高推荐物品的多样性。 论文作者结合两者的优点，提出了一个新的学习算法——Wide & Deep Learning，其中Wide & Deep分别对应Memorization & Generalization。

wide部分就是传统的LR模型 ，LR模型简单、快速并且模型具有可解释，有着很好的拟合能力，但是LR模型是线性模型，表达能力有限，泛化能力较弱，需要做好特征工程，尤其需要交叉特征，才能取得一个良好的效果

deep部分就是DNN ，DNN模型不需要做太精细的特征工程，就可以取得很好的效果。

### 3.deepfm 一定优于 wide & deep 吗
由于DeepFM算法有效地结合了因子分解机与神经网络在特征学习中的优点：同时提取到低阶组合特征与高阶组合特征，所以越来越被广泛使用。在DeepFM中，FM算法负责对一阶特征以及由一阶特征两两组合而成的二阶特征进行特征的提取；DNN算法负责对由输入的一阶特征进行全连接等操作形成的高阶特征进行特征的提取。DeepFM具有以下特点：

1. 结合了广度和深度模型的优点，联合训练FM模型和DNN模型，同时学习低阶特征组合和高阶特征组合。

2. 端到端模型，无需特征工程。

3. DeepFM 共享相同的输入和 embedding vector，训练更高效。


​		DeepFM目的是同时学习低阶和高阶的特征交叉，主要由FM和DNN两部分组成，底部共享同样的输入。

1、wide模型部分由LR替换为FM。FM模型具有自动学习交叉特征的能力，避免了原始Wide & Deep模型中浅层部分人工特征工程的工作。

2、共享原始输入特征。DeepFM模型的原始特征将作为FM和Deep模型部分的共同输入，保证模型特征的准确与一致。

​		Wide&Deep：同时学习低阶和高阶组合特征，它混合了一个线性模型（Wide part）和Deep模型(Deep part)。这两部分模型需要不同的输入，而Wide part部分的输入，依旧依赖人工特征工程。偏向于提取低阶或者高阶的组合特征。不能同时提取这两种类型的特征。
​		在Wide&Deep的基础上进行改进，不需要预训练FM得到隐向量，不需要人工特征工程，能同时学习低阶和高阶的组合特征；FM模块和Deep模块共享Feature Embedding部分，可以更快的训练，以及更精确的训练学习。

1）两者的DNN部分模型结构相同；

2）wide&deep需要做特征工程，二阶特征交叉需要靠特征工程来实现，通过wide部分发挥作用；

3）DeepFM完全不需要做特征工程，直接输入原始特征即可，二阶特征交叉靠FM来实现，并且FM和DNN共享相同的embedding；

4）从试验结果来看DeepFM效果优于wide&deep。



### 4、transformer的position embedding和BERT的position embedding的区别

### 5、如何解决稀疏问题

### 6、特征选择方法

​		特征数量往往较多，其中可能存在不相关的特征，特征之间也可能存在相互依赖，特征个数越多，分析特征、训练模型所需的时间就越长。

​		**序列前向选择**：每次都选择一个使得评价函数的取值达到最优的特征加入，其实就是一种简单的贪心算法。

​		**序列后向选择**：从特征全集O开始，每次从特征集O中剔除一个特征x，使得剔除特征x后评价函数值达到最优。序列后向选择与序列前向选择正好相反，它的缺点是特征只能去除不能加入

### 7、FM与FFM的区别

​		FFM(Field Factorization Machine)是在FM的基础上引入了“场（Field）”的概念而形成的新模型。在FM中计算特征 ![[公式]](https://www.zhihu.com/equation?tex=x_i) 与其他特征的交叉影响时，使用的都是同一个隐向量 ![[公式]](https://www.zhihu.com/equation?tex=V_i) 。而FFM将特征按照事先的规则分为多个场(Field)，特征 ![[公式]](https://www.zhihu.com/equation?tex=x_i)属于某个特定的场f。每个特征将被映射为多个隐向量 ![[公式]](https://www.zhihu.com/equation?tex=V_%7Bi1%7D%2C%E2%80%A6%2CV_%7Bif%7D) ，每个隐向量对应一个场。当两个特征 ![[公式]](https://www.zhihu.com/equation?tex=x_i%2C+x_j) ,组合时，用对方对应的场对应的隐向量做内积:

​		FFM 由于引入了场，使得每两组特征交叉的隐向量都是独立的，可以取得更好的组合效果， FM 可以看做只有一个场的 FFM。

### 8、ItemCF 与 User CF的区别

​		UserCF：推荐那些和他有共同兴趣爱好的用户喜欢的物品。ItemCF：推荐那些和他之前喜欢的物品类似的物品。根据用户推荐重点是反应和用户兴趣相似的小群体的热点，根据物品推荐着重与用户过去的历史兴趣。

​		User：适用于用户较少的场合，如果用户过多，计算用户相似度矩阵的代价交大；效性要求高，用户个性化兴趣要求不高；（实时性）用户有新行为，不一定需要推荐结果立即变化。（冷启动）在新用户对少的物品产生行为后，不能立即对他进行个性化推荐，因为用户相似度是离线计算的新物品上线后一段时间，一旦有用户对物品产生行为，就可以将新物品推荐给其他用户。很难提供推荐理由。

​		Item：适用于物品数明显小于用户数的场合，如果物品很多，计算物品相似度矩阵的代价交大；用户有新行为，一定会导致推荐结果的实时变化；（冷启动）新用户只要对一个物品产生行为，就能推荐相关物品给他，但无法在不离线更新物品相似度表的情况下将新物品推荐给用户。可以根据用户历史行为归纳推荐理由。

### 9、对于ctr/cvr中如果正负比例差异特别大，如何处理？

  1. 采样 2. 调整loss weight

     

### 10、FM特征交叉

​	FM 模型的解决办法是为每个维度的特征 (![[公式]](https://www.zhihu.com/equation?tex=x_i)) 学习一个表征向量 (![[公式]](https://www.zhihu.com/equation?tex=v_i)， 其实可以理解为特征 ID 的 Embedding 向量)。引入辅助向量 ![[公式]](https://www.zhihu.com/equation?tex=V) 最为重要的一点是使得 ![[公式]](https://www.zhihu.com/equation?tex=x_tx_i) 和 ![[公式]](https://www.zhihu.com/equation?tex=x_ix_j) 的参数不再相互独立，这样就能够在样本数据稀疏的情况下合理的估计模型交叉项的参数

​	LR, FM都比较适用于高维稀疏特征, gbdt不适合. FM能够得到特征分量之间的关系(通常是两两特征)

​	时间复杂度为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28n%5E2%29) ，数学上对二次项的化简，因此从数学上对该式最后一项进行一些改写可以把时间复杂度降为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BO%7D%28kn%29)

### 11、多任务/多目标学习，MMOE的特点 

​		多任务学习-Shared-Bottom Multi-task Model，浅层参数共享，互相补充学习。

### 12、Deep&Cross模型的特点 怎么做交叉的

​		在deep&wide模型，wide部分需要手动进行特征工程，deep&cross模型在cross部分实现了自动生成高阶交叉特征，并使用了resnet的形式。

​			对于第 ![[公式]](https://www.zhihu.com/equation?tex=l) 层的输出 ![[公式]](https://www.zhihu.com/equation?tex=x_%7Bl%2B1%7D) ，由原始输入 ![[公式]](https://www.zhihu.com/equation?tex=x_%7B0%7D) 和上一层的输入 ![[公式]](https://www.zhihu.com/equation?tex=x_%7Bl%7D) 相乘得到，从而产生高阶交叉特征。这里使用了resnet的形式 ![[公式]](https://www.zhihu.com/equation?tex=y+%3D+f%28x%29%2Bx) ，使得网络结构更具有鲁棒性



### 13、tf-idf

​		**term frequency–inverse document frequency，词频-逆向文件频率**

​		TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。**字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降**。

​		**TF-IDF的主要思想是**：如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。

​		**词频（TF）****表示词条（关键字）在文本中出现的频率**。**逆向文件频率 (IDF)** ：某一特定词语的IDF，可以由**总文件数目除以包含该词语的文件的数目**，**再将得到的商取对数得到**。
### 14、word2vector
负采样（negative sampling）解决了这个问题，它是用来提高训练速度并且改善所得到词向量的质量的一种方法。不同于原本每个训练样本更新所有的权重，负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。
当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新
### 15将one-hot vector映射为embedding都有哪些方式？
SVD分解、word2vector、bert
### 16如何评估特征重要性？
随机森林、GBDT。基尼系数，信息增益、信息增益率。
    用随机森林进行特征重要性评估的思想其实很简单，说白了就是看看每个特征在随机森林中的每颗树上做了多大的贡献，然后取个平均值，最后比一比特征之间的贡献大小。通常可以用基尼指数（Gini index）或者袋外数据（OOB）错误率作为评价指标来衡量。直观地说，就是随便从节点m mm中随机抽取两个样本，其类别标记不一致的概率。计算分裂前后基尼系数的变化。
    （1）特征工程

特征预处理：ID化、离散化、归一化等；

特征选择：方差、变异系数、相关系数、Information Gain、Information Gain-Ratio、IV值等；

特征交叉和组合特征：根据特征具有的业务属性特征交叉，利用FM算法、GBDT算法做高维组合特征等
### 17、Bert？    
  token嵌入层的作用是将单词转换为固定维的向量表示形式。Segment嵌入。Position嵌入.
  Transformer的多头注意力看上去是借鉴了CNN中同一卷积层内使用多个卷积核的思想，原文中使用了 8 个 scaled dot-product attention ，在同一 multi-head attention 层中，输入均为 KQV ，同时进行注意力的计算，彼此之前参数不共享，最终将结果拼接起来，这样可以允许模型在不同的表示子空间里学习到相关的信息
  
### 18召回算法的策略？
召回策略主要包含两大类，即基于内容匹配的召回和基于系统过滤的召回。
内容匹配即将用户画像与内容画像进行匹配，又分为基于内容标签的匹配和基于知识的匹配。
例如，A用户的用户画像中有一条标签是“杨幂的粉丝”，那么在他看了《绣春刀2》这部杨幂主演的电影后，可以为他推荐杨幂主演的其他电影或电视剧，这就是“基于内容标签的匹配”。
基于用户（User-based）的协同推荐是最基础的，它的基础假设是“相似的人会有相同的喜好”，推荐方法是，发现与用户相似的其他用户，用用户的浏览记录做相互推荐。
基于模型的协同过滤推荐（Model-based）就是基于样本的用户喜好信息，训练一个推荐模型，然后根据实时的用户喜好的信息进行预测推荐。
基于项目（Item-based）的协同过滤中的“项目”可以视场景定为信息流产品中的“内容”或者电商平台中的“商品”，其基础假设是“喜欢一个物品的用户会喜欢相似的物品”计算项目之间的相似性，再根据用户的历史偏好信息将类似的物品推荐给该用户。
基于协同过滤的召回即建立用户和内容间的行为矩阵，依据“相似性”进行分发。这种方式准确率较高，但存在一定程度的冷启动问题。

### 19为什么负样本只在batch内执行？

YouTube这个模型最大的不同是，它的训练是基于流数据的，每一天都会产生新的训练数据。

因此，负样本的选择只能在batch内进行，batch内的所有样本作为彼此的负样本去做batch softmax。这种采样的方式带来了非常大的bias。一条热门视频，它的采样概率更高，因此会更多地被当做负样本，这不符合实际。因此这篇工作的核心就是减小batch内负采样带来的bias。

