自我介绍：

项目一：工业图像：

​	亮度调整、二值化、白平衡、归一化。stl分解：将时间序列分解成趋势分量、季节分量和残差项。

​	太阳能能电池板，长方形，里面多条规则的栅线，异常为不规则的裂纹。为了语义分割时候，更明确的区分栅线和裂纹。用stl分解的方法，去除了栅线。同时增强了异常区域和正常区域的对比度。

​	11个卷积层，每一个卷积层后+特征归一化+Relu，2（32通道，5*5）、3（64通道，5*5））、4（64通道，5*5）、1（1024通道，15*15）、1（1*1）。输出图的大小是原图的1/8。

目标函数：交叉熵损失函数和均方误差的结合。

​	   提出的分割网络的设计重点是检测大分辨率图像中的微小表面缺陷。为了实现这一点，网络的设计有两个重要的要求：（a）在高分辨率图像中需要一个大的感受野大小；（b）需要捕获小的特征细节。与Racˇki等人（2018）的相关工作相比，这导致了架构的一些重大变化。首先，一个额外的下采样层和较大的内核尺寸在更高的层被用来显着增加感受野的大小。第二，每个下采样之间的层的数量，在架构的较低部分中具有较少的层并且在较高部分中具有更多的层。这增加了具有较大接收野大小的特征的容量。最后，用最大池代替大步长的卷积实现下采样。这确保了小而重要的细节在下采样过程中幸存下来，这在具有额外下采样层的网络中尤其重要。

​	分类网络：语义分割结果 全局最大池化层+全局平均池化层 = 1*2； 语义分割结果与1024通道进行拼接。继续卷积+池化层。 32通道的数据 继续进行  全局最大池化层+全局平均池化层 =1**64， 66 个进行全连接层。输出异常的概率。 目标函数：交叉熵损失函数。	

​	决策网络的设计遵循两个重要原则。首先，通过使用几层卷积和下采样来确保大型复杂形状的适当容量。这使得网络不仅能够捕获局部形状，而且能够捕获跨越图像大面积的全局形状。其次，该决策网络不仅利用了1×1核信道约简前分割网络最后一次卷积运算的输出特征量，还利用了1×1核信道约简后得到的最终分割输出图。这引入了一个快捷方式，如果不需要，网络可以利用它来避免使用大量的特征图。它还减少了大量参数的过拟合。该方法在两个层次上实现：一个是在决策网络的开头，将分割输出映射输入到决策网络的几个卷积层中；另一个是在决策网络的结尾，将分割输出映射的全局平均值和最大值附加到决策网络的输入中最后一个完全连接的层。决策网络开始处的捷径和几个具有下采样的卷积层是Racˇki等人（2018）相关工作的重要区别。与所提出的工作不同，它们在决策层中只使用一层，不使用下采样，在卷积中不直接使用分割输出映射，而是通过全局最大值和平均池间接使用分割输出映射。这限制了决策网络的复杂性，并阻止它捕获大的全局形状



数据预处理：亮度调整、二值化、白平衡、归一化

（基于小样本的的分类方法。训练时序需要多种类别，学会的是区分不同类别的泛化能力，不适合与本场景。自监督的异常检测方法，）

1.数据量少（800张）：数据增强（翻转、旋转、拼接、加噪声），基于小样本的语义分割网络。

2.轻微异常比较难检测，且容易与角落和姗线混淆。	stl分解，增强与异常区域的对比度，去除姗线的影响。

3.语义分割网络。

4.构建了分类网络，四个卷积层，两个全连接层，较简单，避免过拟合。加上分类网络可以，人工构造更多的数据集。 交叉熵损失函数。

​      自助法采样多个数据集，训练多个分类模型。计算平均最优的异常分数阈值。然后再用数据重新训练模型，选择此前的异常分数做为预知。



## 项目二：时间序列的异常点检测：

​		时间异常检测背景，人和机器出现故障的时候。单变量和多变量的时间序列异常检测方法。

​		异常的定义是根据具体的问题定义，什么是异常，并没有标准答案，通常因具体应用场景而异。 异常数据跟样本中大多数数据不太一样。 异常数据在整体数据样本中占比比较小。

​		1、突变异常和上下文异常。2、 时间序列有周期和趋势性。去除周期再检测异常，STL分解方法需要事先知道周期。1）谱残差去除周期2）LSTM+attention捕捉序列的上下文关系。

​		传统的基于距离的异常检测技术并没有时间序列的上下文，正因为这样，他们不能找到周期内的点异常。我们提出无监督方法在检测异常时考虑了上下文、周期和趋势。这个方法适用于不同的场景和应用案例，并且对不同领域的数据也同样有效。

​	 **基于统计的异常检测**

​	 许多异常检测技术首先建立一个数据模型。异常是那些同模型不能完美拟合的对象。例如，数据分布模型可以通过估计概率分布的参数来创建。如果一个对象不能很好地同该模型拟合，即如果它很可能不服从该分布，则它是一个异常。如果模型是簇的集合，则异常是不显著属于任何簇的对象。在使用回归模型时，异常是相对远离预测值的对象。

​			ARMA

​		当前时刻的时序值可由其过去值的p阶线性组合，加上一个白噪声q 阶白噪声的线性组合。ARMA模型是平稳的时间序列模型，在建模前必须去除趋势性。序列相关性是指当期的序列值和前期某个或某些序列值线性相关。随机性是指序列在一定程度上呈现不确定性，由于模型并不能捕捉到现实世界中的所有特征，总会有一些噪声的存在，这些噪声我们称之为白噪声。

​		显著性检测技术：谱残差法是模拟人类的视觉注意机制的视觉显著性检测技术，可以有效地去除图像信息中的背景信息而保留显著性部分，有利于检测图像中的异常区域。KDD2019认为视觉显著性检测和时间序列异常检测任务在本质上是相似的，因为异常点通常在视觉视角下是显著的。

### 2.2谱残差变化过程		

​		将时间序列进行一维的傅立叶变换，得到幅度谱A和相位谱P，计算对数幅度谱L。用均值滤波器对幅度谱进行滤波，得道时间序列的冗余信息V。计算残差L--V。将残差和相位谱用逆傅立叶变换得到显著序列。（均值q取之太小，会导致冗余信息包含显著性信息，q过大，得到的冗余部分不完整，就会引入噪声）。

​		自然界里的图像的对数幅度谱都是一个相似的形状。剩余谱理论认为这个相似性就是图像的冗余所在。而上下的波动就是图像变化剧烈的区域，也就是显著性区域。

​		幅度谱就是，横轴是频率，纵轴是振幅相位谱就是，横轴是频率，纵轴的相位

​		均值滤波是3x3的，因为一般都是奇数，常用的1,3,9等，cnn（和全连接差不多，权值共享，局部感知）

### 2.3其它图像显著性检测方法

​		LC算法的基本思想是：计算某个像素在整个图像上的全局对比度，即该像素与图像中其他所有像素在颜色上的距离之和作为该像素的显著值

​		AC算法是基于局部对比度的，采用Lab颜色空间计算距离。AC算法通过计算一个感知单元在不同邻域上的局部对比度来实现多尺度显著性计算。

相位谱（只对相位谱进行傅里叶逆变换，缺点剔除了太多背景）

### 2.4数据集及处理方法

​		心跳、体温数据集，自己插值的制造了部分异常点。

​		是yahoo公司发布的时间序列异常检测的数据集，是最近非常流行的。分为4个，第一个是yahoo服务器产生的真实数据，人工标签。另外3个都是人工合成的。
​		特点：每一条序列，异常点很少，比例可以达到1000:1。有做了一些插值处理，但结果证明用处不大，因为希望捕捉的是正常模式，异常点增多反而不利于训练，许多论文甚至只用正常数据进行训练。

​		归一化处理。最大最小化归一化，z-score的话对异常点不敏感。

### 2.5实验参数和训练过程

​		隐藏层神经元数量是16，都是试出来的，一般是2的幂，这个数字就是编码解码的中间向量的大小，太大了会过拟合，小了可能欠拟合。使用Adam优化器，均方误差损失函数。xavier_normal初始化参数。

​		超参数的选择有网格搜索（穷举），随机搜索等。（对于大多数数据集，只有少数超参数真的很重要，但不同的超参数对于不同的数据集很重要）

​		每个epoch之后，评估测试集表现，判断何时停止（提前停止）。学习速率是最为重要的超参数。数据标准化、MSE损失函数 。xavier_normal初始化参数。堆叠层和隐藏层数量。

### 2.6对比方法

​		ARMA

​		当前时刻的时序值可由其过去值的p阶线性组合，加上一个白噪声q 阶白噪声的线性组合。ARMA模型是平稳的时间序列模型，在建模前必须去除趋势性。序列相关性是指当期的序列值和前期某个或某些序列值线性相关。随机性是指序列在一定程度上呈现不确定性，由于模型并不能捕捉到现实世界中的所有特征，总会有一些噪声的存在，这些噪声我们称之为白噪声。

​		WaveNet（一维度的膨胀因果卷积）、

​		基于LSTM和GRU的编码器。基于STL分解的方法。

​		消融实验的：去除谱残差。讲谱残差换位其它方法。采用其它的编码-解码器结构。去掉attention机制。

​		统计学方法对数据的正常性做出假定。**它们假定正常的数据对象由一个统计模型产生，而不遵守该模型的数据是异常点。**统计学方法的有效性高度依赖于对给定数据所做的统计模型假定是否成立。异常检测的统计学方法的一般思想是：学习一个拟合给定数据集的生成模型，然后识别该模型低概率区域中的对象，把它们作为异常点。

​		傅立叶变换：傅里叶提出任何周期函数都可以表示为不同频率的正弦和的形式，每个正弦和乘以不同的系数。	傅里叶变换，则是将一个时域非周期的连续信号，转换为一个在频域非周期的连续信号。（傅里叶变换实际上是对一个周期无限大的函数进行傅里叶变换。）

​		在傅里叶分析中，把各个分量的幅度|Fn|或 Cn 随着频率nω1的变化称为信号的**幅度谱**。而把各个分量的相位 φn 随角频率 nω1 变化称为信号的**相位谱**。幅度谱和相位谱统称为信号的频谱

​		图像的频率是表征图像中灰度变化剧烈程度的指标，是灰度在平面空间上的梯度.在噪声点和图像边缘处的频率为高频。傅立叶变换的物理意义是将图像的灰度分布函数变换为图像的频率分布函数，傅立叶逆变换是将图像的频率分布函数变换为灰度分布函数.

## 项目三：异常时间序列检测：

​		分析，异常的

​		时间序列编码是指对时间序列进行一系列的变换，通过对变换后的数据进行处理和分析，使得模型能更好的捕捉时间序列的复杂模型。可以采用图像领域发展日新月异，用处理二维图像的方法处理一维的数据上。

​		首先对原始的时序数据进行离散余弦变换，把数据从时域转化到频域，然后将处理后的数据转化为特征图形式；用U-Net对特征图形式的时间序列进行编码和解码，在编码过程中捕捉时序数据的复杂特征和正常模式，同时将其压缩到较低维的潜在空间中；在解码过程中网络将根据输入数据和学习到的数据的正常模式，尽可能地重构其输入；训练好的U-Net模型具有很强的重构能力，如果输入的是正常的数据，其重构误差较小，反之则重构误差较大，因此可以根据重构误差判断数据是否异常。

​		离散余弦变换是与傅立叶变换相关的一种变换，常用于对信号和图像进行有损数据压缩，离散余弦变换后的数据能量集中在低频部分。

​		太深的卷积神经网络拟合能力过强，只用了四层。通道数目减少，采用更多等卷积核扩大感受野。

### 数据集

​		空中客车公司飞机震动数据。 脑磁波和脑电波序列。

### 对比方法

​		消融实验：不用离散余弦变换、采用傅立叶变换的。更换编码器

​		对比实验：KNN-ed、基于LSTM的、一维卷积DeepAnT

## 常用异常检测方法：

   	什么是异常，并没有标准答案，通常因具体应用场景而异。异常数据跟样本中大多数数据不太一样。
异常数据在整体数据样本中占比比较小。

​		为了刻画异常数据的“不一样”，最直接的做法是利用各种统计的、距离的、密度的量化指标去描述数据样本跟其他样本的疏离程度。

​		基于统计的异常检测算法通常需要假设数据服从特定的概率分布，这个假设往往是不成立的。而聚类的方法通常只能给出 0/1 的判断（即：是不是异常点），不能量化每个数据点的异常程度。相比较而言，基于密度的LOF算法要更简单、直观。它不需要对数据的分布做太多要求，还能量化每个数据点的异常程度（outlierness）。

### 1、Isolation Forest

​		而 Isolation Forest 的想法要巧妙一些，它尝试直接去刻画数据的“疏离”程度，而不借助其他量化指标。Isolation Forest 因为简单、高效，在学术界和工业界都有着不错的名声。

​		Isolation Forest 算法主要有两个参数：一个是二叉树的个数；另一个是训练单棵 iTree 时候抽取样本的数目。

​		Isolation Forest 是无监督的异常检测算法，在实际应用时，并不需要黑白标签。需要注意的是：（1）如果训练样本中异常样本的比例比较高，违背了先前提到的异常检测的基本假设，可能最终的效果会受影响；（2）异常检测跟具体的应用场景紧密相关，算法检测出的“异常”不一定是我们实际想要的。比如，在识别虚假交易时，异常的交易未必就是虚假的交易。所以，在特征选择时，可能需要过滤不太相关的特征，以免识别出一些不太相关的“异常”

### 2、LOF局部异常因子

​		根据局部可达密度的定义，如果一个数据点跟其他点比较疏远的话，那么显然它的局部可达密度就小。但LOF算法衡量一个数据点的异常程度，并不是看它的绝对局部密度，而是看它跟周围邻近的数据点的相对密度。这样做的好处是可以允许数据分布不均匀、密度不同的情况。局部异常因子即是用局部相对密度来定义的。

		1. 对于每个数据点，计算它与其它所有点的距离，并按从近到远排序；、
		2. 对于每个数据点，找到它的 k-nearest-neighbor，计算 LOF 得分。

## 毕设论文：

AlexNet（网络8层，5层卷积，3层全连接层）

​	1）成功使用ReLU作为CNN的激活函数，并验证其效果在较深的网络超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题。

（2）训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合。

（3）在CNN中使用的最大池化。此前CNN中普遍使用平均池化，AlexNet全部使用最大池化，避免平均池化的模糊化效果。

（4）提出了LRN层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。LRN的原因是为了鼓励**横向抑制（lateral inhibition）**，是指神经元减少其邻居活动的能力[1]（***注：阻止兴奋神经元向邻近神经元传播其动作趋势，从而减少兴奋神经元的邻近神经元的\**程度***）。在深度神经网络，这种横向抑制的目的是进行局部对比度增强，以便使局部最大像素值用作下一层的激励。LRN是一个不可训练的层，对局部邻域内的特征图中的像素值进行平方归一化

（5）使用CUDA加速深度卷积网络的训练，利用GPU强大的并行计算能力，处理神经网络训练时大量的矩阵运算。AlexNet使用了两块GTX 580 GPU进行训练，单个GTX 580只有3GB显存，这限制了可训练的网络的最大规模。因此作者将AlexNet分布在两个GPU上，在每个GPU的显存中储存一半的神经元的参数。因为GPU之间通信方便，可以互相访问显存，而不需要通过主机内存，所以同时使用多块GPU也是非常高效的。同时，AlexNet的设计让GPU之间的通信只在网络的某些层进行，控制了通信的性能损耗。 

（6）数据增强，随机地从256*256的原始图像中截取224*224大小的区域（以及水平翻转的镜像）

#### VGG-16（16个卷积层和全连接层，1.38亿个参数）

（1）采用尺寸较小的3x3卷积核，通过padding对卷积结果填充，保证卷积后特征图尺寸和前层保持一致。

（2）通过不断增加通道数达到更深的网络，通过池化层降低特征图尺寸为前一层的一半。

*网络深，卷积核小，池化核小（与AlexNet的3*3池化核相比，VGG全部用的是2*2的池化层）*

*全连接转卷积。（网络测试阶段将训练阶段的三个全连接替换为三个卷积（1个conv7x7，2个conv1x1），测试重用训练时的参数，使得测试得到的全卷积网络因为没有全连接的限制，因而可以接收任意宽和高的输入。）
*

***优点：**
（1）层数深使得特征图更宽，更加适合于大的数据集，该网络可以解决1000类图像分类和定位问题。
（2）卷积核的大小影响到了参数量，感受野，前者关系到训练的难易以及是否方便部署到移动端等，后者关系到参数的更新、特征图的大小、特征是否提取的足够多、模型的复杂程度。
（VGG用较深的网络结构和较小的卷积核既可以保证感受视野，又能够减少卷积层的参数，比如两个3*3的卷积层叠加等价于一个5*5卷积核的效果，3个3*3卷积核叠加相加相当于一个7*7的卷积核，而且参数更少。大约是7*7卷积层的（3*3*3）/（7*7）=0.55，三个卷积层的叠加，对特征学习能力更强）
（3）池化层：从AlexNet的kernel size为3*3，stride为2的max-pooling改变为kernel size均为2*2，stride为2的max-pooling，小的池化核能够带来更细节的信息捕获（当时也有average pooling，但是在图像任务上max-pooling的效果更好，max更加容易捕捉图像上的变化，带来更大的局部信息差异性，更好的描述边缘纹理等，用average-pooling可能会使得图像模糊了，类似与数字图像处理的高斯模糊）*

# Inception V1

​		首先通过1x1卷积来降低通道数把信息聚集一下，再进行不同尺度的特征提取以及池化，得到多个尺度的信息，最后将特征进行叠加输出。把不同的卷积核组合在一起，不仅可以增大感受野，而且还可以提高神经网络的鲁棒性

V2

​		将 5×5 的卷积分解为两个 3×3 的卷积运算以提升计算速度。如此可以有效地只使用约(3x3 + 3x3)/(5x5)=72%的计算开销

​		将 n*n 的卷积核尺寸分解为 1×n 和 n×1 两个卷积。

## **Inception v3**

Inception v3 整合了前面 Inception v2 中提到的所有升级，还使用了：

- RMSProp 优化器； 
- Factorized 7x7 卷积； 
- 辅助分类器使用了 BatchNorm； 
- 标签平滑（添加到损失公式的一种正则化项，旨在阻止网络对某一类别过分自信，即阻止过拟合）