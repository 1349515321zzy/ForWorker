# LR，偏置项的作用

​		**逻辑回归假设数据服从伯努利分布，通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。**逻辑回归的**第二个假设**是正类的概率由sigmoid的函数计算。
没有偏置项 b，那么就只能在空间里画过原点的直线/平面/超平面。这时对于绝大部分情况，要求决策面过原点，加上这个偏置项b，才能保证分类器可以在空间的任何位置画决策面

​		LR的损失函数：最大似然损失函数（MLE，即另每个样本属于其真实标记的概率越大越好）

2.LR的优点：

​		直接对分类的可能性进行建模，无需事先假设数据分布，避免了假设数据分布不准确带来的问题。

​		不仅预测出类别，而且可以得到近似概率的预测。LR求解的目标函数是任意阶可导的凸函数，有很好的数学性质。（梯度下降法、牛顿法都可以求解）

缺点：很难处理数据不平衡的问题；准确率并不是很高；处理非线性数据较麻烦；本身无法筛选特征

3.LR的决策边界是平面还是曲面，原因？

​	决策边界由两个东西决定的：（1）方程的形式；（2）方程的参数。对特征的线性组合，参数均为一次，所以决策边界为平面。

4.逻辑回归的并行处理：

#### ● LR和线性回归的区别 

​		线性回归用来做预测，LR用来做分类。线性回归是来拟合函数,LR是来预测函数。线性回归用最小二乘法来计算参数，LR用最大似然估计来计算参数。线性回归更容易受到异常值的影响,而LR对异常值有较好的稳定性。

### 线性回归

​		通过属性的线性组合来预测实值的函数。形式简单，易于建模，可解释性强。 可通过引入层级结构或高维映射得到非线性模型。

​		连续特征的离散化，特征交叉，gbdt交叉特征提取。可以增强模型的表达能力。

1.线性回归为什么要使用最小二乘法

​		“最小二乘法”的思想，“二乘”指的是用平方来度量观测点与估计点的距离（远近），“最小”指的是参数值要保证各个观测点与估计点的距离的平方和达到最小。**最小二乘法以估计值与观测值的平方和作为损失函数，在误差服从正态分布的前提下，与极大似然估计的思想在本质上是相同**



## SVM

​		使用hinge loss不仅要分类正确，而且置信度足够高的时候，损失才为0，对学习有更高的要求。

​		在统计学中，参数模型通常假设总体（随机变量）服从某一个分布，该分布由一些参数确定（比如正太分布由均值和方差确定），在此基础上构建的模型称为参数模型；非参数模型对于总体的分布不做任何假设，只是知道总体是一个随机变量，其分布是存在的（分布中也可能存在参数），但是无法知道其分布的形式，更不知道分布的相关参数，只有在给定一些样本的条件下，能够依据非参数统计的方法进行推断。

#### ● LR和SVM的区别 

​		1）LR是参数模型，SVM是非参数模型。2）从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。3）SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。4）逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。5）logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。

#### **2. 简单概括一下SVM：**

​		SVM 是一种二类分类模型。它的基本思想是在特征空间中寻找间隔最大的分离超平面使数据得到高效的二分类，具体来讲，有三种情况（不加核函数的话就是个线性模型，加了之后才会升级为一个非线性模型）：

- 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；
- 当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；
- 当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。

#### **3. SVM 为什么采用间隔最大化（与感知机的区别）：**

​		当训练数据线性可分时，存在无穷个分离超平面可以将两类数据正确分开。感知机利用误分类最小策略，求得分离超平面，不过此时的解有无穷多个。线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强。

#### **4. SVM的目标（硬间隔）：**

有两个目标：第一个是**使间隔最大化**，第二个是**使样本正确分类，**由此推出目标函数：

![[公式]](https://www.zhihu.com/equation?tex=%E7%9B%AE%E6%A0%87%E4%B8%80%EF%BC%88%E4%BD%BF%E9%97%B4%E9%9A%94%E6%9C%80%E5%A4%A7%E5%8C%96%EF%BC%89%EF%BC%9A%7B%5Cmin+_%7B%5Cmathbf%7Bw%7D%2C+b%7D+%5Cfrac%7B1%7D%7B2%7D%5C%7C%5Cmathbf%7Bw%7D%5C%7C%5E%7B2%7D%7D%5C%5C)

![[公式]](https://www.zhihu.com/equation?tex=%E7%9B%AE%E6%A0%87%E4%BA%8C%EF%BC%88%E4%BD%BF%E6%A0%B7%E6%9C%AC%E6%AD%A3%E7%A1%AE%E5%88%86%E7%B1%BB%EF%BC%89%EF%BC%9Ay_%7Bi%7D%5Cleft%28%5Cmathbf%7Bw%7D%5E%7BT%7D%5Cmathbf%7Bx%7D_i%2Bb%5Cright%29+%5Cgeq+1%2C+i%3D1%2C2%2C+%5Cldots%2C+m%5C%5C)

稍微解释一下，w是超平面参数，目标一是从点到面的距离公式化简来的，目标二就相当于感知机，只是把大于等于0进行缩放变成了大于等于1，为了后面的推导方便。有了两个目标，写在一起，就变成了svm的终极目标：

![[公式]](https://www.zhihu.com/equation?tex=%E7%BB%88%E6%9E%81%E7%9B%AE%E6%A0%87%EF%BC%9A%5Cbegin%7Barray%7D%7Bc%7D%7B%5Cmin+_%7Bw%2C+b%7D+%5Cfrac%7B1%7D%7B2%7D%5C%7Cw%5C%7C%5E%7B2%7D%7D+%5C%5C+%7B%5Ctext+%7Bs.t.+%7D+y_%7Bi%7D%5Cleft%28w%5E%7BT%7D+x_%7Bi%7D%2Bb%5Cright%29+%5Cgeq+1%2C+%5Cforall+i%7D%5Cend%7Barray%7D%5C%5C)

**利用SMO（序列最小优化）算法**：

​	SMO算法的基本思路是每次选择两个变量![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_%7Bi%7D)和![[公式]](https://www.zhihu.com/equation?tex=%5Calpha_%7Bj%7D)，选取的两个变量所对应的样本之间间隔要尽可能大，因为这样更新会带给目标函数值更大的变化。**SMO算法之所以高效，是因为仅优化两个参数的过程实际上仅有一个约束条件，其中一个可由另一个表示，这样的二次规划问题具有闭式解。**

**为什么SVM对缺失数据敏感？**

​        这里说的缺失数据是指缺失某些特征数据，向量数据不完整。SVM 没有处理缺失值的策略。而 SVM 希望样本在特征空间中线性可分，所以特征空间的好坏对SVM的性能很重要。缺失特征数据将影响训练结果的好坏。

#### 5.核函数的作用和选择

​		核函数隐含着一个从低维空间到高维空间的映射,这个映射可以把低维空间中线性不可分的两类点变成线性可分的。

​		1. 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM

​		2.如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel

​		如果特征维数很高，往往线性可分（SVM 解决非线性分类问题的思路就是将样本映射到更高维的特征空间中），可以采用 LR 或者线性核的 SVM；

- 如果样本数量很多，由于求解最优化问题的时候，目标函数涉及两两样本计算内积，使用高斯核明显计算量会大于线性核，所以手动添加一些特征，使得线性可分，然后可以用 LR 或者线性核的 SVM；
- 如果不满足上述两点，即特征维数少，样本数量正常，可以使用高斯核的 SVM。



#### 6. SVM为什么使用对偶函数求解

​	  对偶将原始问题中的不等式约束转为了对偶问题中的等式约束,而且更加方便了核函数的引入,同时也改变了问题的复杂度,在原始问题下,求解问题的复杂度只与样本的维度有关,在对偶问题下,只与样本的数量有关。

**十三. SVM的优缺点：**

**优点：**

1. 由于SVM是一个凸优化问题，所以求得的解一定是全局最优而不是局部最优。
2. 不仅适用于线性线性问题还适用于非线性问题(用核技巧)。
3. 拥有高维样本空间的数据也能用SVM，这是因为数据集的复杂度只取决于支持向量而不是数据集的维度，这在某种意义上避免了“维数灾难”。
4. 理论基础比较完善(例如神经网络就更像一个黑盒子)。

**缺点：**

1. 二次规划问题求解将涉及m阶矩阵的计算(m为样本的个数), 因此SVM不适用于超大数据集。(SMO算法可以缓解这个问题)
2. 只适用于二分类问题。(SVM的推广SVR也适用于回归问题；可以通过多个SVM的组合来解决多分类问题)

## 决策树

​	优点：

1. 天然的可解释性。 这是决策树最大的优点了。 可解释性有两方面的考虑。 一方面， 树结构的理解不需要机器学习专家来解读。 另一方面， 很容易转化成规则。
2. 可以处理缺失值（missing）， 字符型（nominal）， 数值型（numeric）等数据类型。
3. 非参数模型（non-parametric）。 没有复杂的参数设置，谁跑效果都相对一样。
4. 对相关（Correlation）属性能够比较好的处理。
5. 运算速度相对比较快。。

​	1.ID3。从信息论的知识中我们知道：信息熵越大，从而样本纯度越低，。ID3 算法的核心思想就是以信息增益来度量特征选择，选择信息增益最大的特征进行分裂。算法采用自顶向下的贪婪搜索遍历可能的决策树空间（C4.5 也是贪婪搜索）

- ID3 没有剪枝策略，容易过拟合；

- 信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1；

- 只能用于处理离散分布的特征；

- 没有考虑缺失值。

  ## 2. C4.5

  C4.5 算法最大的特点是克服了 ID3 对特征数目的偏重这一缺点，引入信息增益率来作为分类标准。

  ### 2.1 思想

  C4.5 相对于 ID3 的缺点对应有以下改进方式： 

  - 引入悲观剪枝策略进行后剪枝； 
  - 引入信息增益率作为划分标准； 
  - 将连续特征离散化，假设 n 个样本的连续特征 A 有 m 个取值，C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点； 
  - 对于缺失值的处理可以分为两个子问题：
  - 问题一：在特征值缺失的情况下进行划分特征的选择？（即如何计算特征的信息增益率）
  - 问题二：选定该划分特征，对于缺失该特征值的样本如何处理？（即到底把这个样本划分到哪个结点里） 
  - 针对问题一，C4.5 的做法是：对于具有缺失值特征，用没有缺失的样本子集所占比重来折算；
- 将样本同时划分到所有子节点，不过要调整样本的权重值，其实也就是以不同概率划分到不同节点中
  
### 2.4 缺点

- 剪枝策略可以再优化；
  
- C4.5 用的是多叉树，用二叉树效率更高；
  
- C4.5 只能用于分类；
  
- C4.5 使用的熵模型拥有大量耗时的对数运算，连续值还有排序运算；
  
- C4.5 在构造树的过程中，对数值属性值需要按照其大小进行排序，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行。
  
  ## 3. CART
  
  ID3 和 C4.5 生成的决策树分支、规模都比较大，CART 算法的二分法可以简化决策树的规模，提高生成决策树的效率。
  
  ### 3.1 思想
  
  CART 包含的基本过程有分裂，剪枝和树选择。 
  
    - **分裂：**分裂过程是一个二叉递归划分过程，其输入和预测特征既可以是连续型的也可以是离散型的，CART 没有停止准则，会一直生长下去； 
    - **剪枝：**采用**代价复杂度剪枝**，从最大树开始，每次选择训练数据熵对整体性能贡献最小的那个分裂节点作为下一个剪枝对象，直到只剩下根节点。CART 会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树； 
  - **树选择：**用单独的测试集评估每棵剪枝树的预测性能（也可以用交叉验证）。
  
  CART 在 C4.5 的基础上进行了很多提升。 
  
    - C4.5 为多叉树，运算速度慢，CART 为二叉树，运算速度快； 
    - C4.5 只能分类，CART 既可以分类也可以回归； 
    - CART 使用 Gini 系数作为变量的不纯度量，减少了大量的对数运算； 
    - CART 采用代理测试来估计缺失值，而 C4.5 以不同概率划分到不同节点中； 
    - CART 采用“基于代价复杂度剪枝”方法进行剪枝，而 C4.5 采用悲观剪枝方法。

**划分标准的差异：**ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。

**使用场景的差异：**ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；

**样本数据的差异：**ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；

**样本特征的差异：**ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征；

**剪枝策略的差异：**ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。

LDA与PCA

### 1.LDA线性判别分析

​		LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。LDA的基本思想：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点中心尽可能远离。更简单的概括为一句话，就是“投影后类内方差最小，类间方差最大”。

​			**类内散度矩阵**”（within-class scatter matrix）：

![[公式]](https://www.zhihu.com/equation?tex=S_w%3D%5CSigma_0%2B%5CSigma_1)

![[公式]](https://www.zhihu.com/equation?tex=%5CSigma_i%3D%5Csum_%7Bx%5Cin+X_i%7D%28x-%5Cmu_i%29%28x-%5Cmu_i%29%5E%7B%5Ctop%7D)

​			**类间散度矩阵**”（between-class scatter matrix） ![[公式]](https://www.zhihu.com/equation?tex=S_b) ：

![[公式]](https://www.zhihu.com/equation?tex=%7C%7Cw%5E%7B%5Ctop%7D%5Cmu_0-w%5E%7B%5Ctop%7D%5Cmu_1%7C%7C%5E2_2%3Dw%5E%7B%5Ctop%7D%28%5Cmu_0-%5Cmu_1%29%28%5Cmu_0-%5Cmu_1%29%5E%7B%5Ctop%7Dw)

![[公式]](https://www.zhihu.com/equation?tex=J%28w%29%3D%5Cfrac%7Bw%5E%7B%5Ctop%7DS_bw%7D%7Bw%5E%7B%5Ctop%7DS_ww%7D)

**如何确定 ![[公式]](https://www.zhihu.com/equation?tex=w) 呢？**

注意到 ![[公式]](https://www.zhihu.com/equation?tex=J%28w%29) 式中的分子和分母都是关于 ![[公式]](https://www.zhihu.com/equation?tex=w) 的二次项，因此 ![[公式]](https://www.zhihu.com/equation?tex=J%28w%29) 的解与 ![[公式]](https://www.zhihu.com/equation?tex=w) 的长度无关，只与其方向有关（ ![[公式]](https://www.zhihu.com/equation?tex=w) 为投影后直线的方向），不失一般性，令 ![[公式]](https://www.zhihu.com/equation?tex=w%5E%7B%5Ctop%7DS_ww%3D1) ，则式 ![[公式]](https://www.zhihu.com/equation?tex=J%28w%29) 等价于：

![[公式]](https://www.zhihu.com/equation?tex=%5Cmin_%7Bw%7D+-w%5E%7B%5Ctop%7DS_bw)

![[公式]](https://www.zhihu.com/equation?tex=s.t.+%5C+%5C+w%5E%7B%5Ctop%7DS_ww%3D1)

由拉格朗日乘子法，上式等价于：

![[公式]](https://www.zhihu.com/equation?tex=c%28w%29%3D-w%5E%7B%5Ctop%7DS_bw%2B%5Clambda%28w%5E%7B%5Ctop%7DS_ww-1%29%5C%5C%5CRightarrow%5Cfrac%7Bd%7Bc%7D%7D%7Bd%7Bw%7D%7D%3D-2S_bw%2B2%5Clambda+S_ww%3D0%5C%5C%5CRightarrow%5C+%5C+S_bw%3D%5Clambda+S_ww%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+)

其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda) 为拉格朗日乘子。注意到 ![[公式]](https://www.zhihu.com/equation?tex=S_bw) 的方向恒为 ![[公式]](https://www.zhihu.com/equation?tex=%5Cmu_0-%5Cmu_1) ，不妨令：

![[公式]](https://www.zhihu.com/equation?tex=S_bw%3D%5Clambda%28%5Cmu_0-%5Cmu_1%29)

将其带入上式得：

![[公式]](https://www.zhihu.com/equation?tex=w%3DS_w%5E%7B-1%7D%28%5Cmu_0-%5Cmu_1%29)

也就是说我们只要求出原始二类样本的均值和方差就可以确定最佳的投影方向 ![[公式]](https://www.zhihu.com/equation?tex=w) 了。

**LDA算法的优点**

- 在降维过程中可以使用类别的先验知识经验，而像PCA这样的无监督学习无法使用先验知识；
- LDA在样本分类信息依赖均值而不是方差的时候，比PCA算法较优。

**LDA算法的缺点**

- LDA与PCA均不适合对非高斯分布样本进行降维，LDA假设数据符合高斯分布
- LDA降维算法最多降到类别数K-1的维度，当降维的维度大于K-1时，则不能使用LDA。当然目前有一些改进的LDA算法可以绕过这个问题
- LDA在样本分类信息依赖方差而非均值的时候，降维效果不好
- LDA可能过度拟合数据

### 1.LDA于PCA的异同点

LDA与PCA都可用于降维，因此有很多相同的地方，也有很多不同的地方

**相同点：**

- 两者均可用于数据降维
- 两者在降维时均使用了矩阵特征分解的思想
- 两者都假设数据符合高斯分布

**不同点：**

- LDA是有监督的降维方法，而PCA是无监督降维方法
- 当总共有K个类别时，LDA最多降到K-1维，而PCA没有这个限制
- LDA除了用于降维，还可以用于分类
- LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。这点可以从下图形象的看出，在某些数据分布下LDA比PCA降维较优（如下图的左图）。当然，某些数据分布下PCA比LDA降维较优（如下图的右图）。

## 朴素贝叶斯分类器

1.P(x|c)是所有属性上的联合概率，难以从有限训练样本估计联合概率，在计算上会遭遇组合爆炸的问题，在数据上会遇到样本稀疏问题，属性越多，问题越严重。为避开这个障碍，采用属性条件独立条件假设。

​	训练过程就是基于训练集合计算类的先验概率，并为每个属性估计条件概率P(xi|c)

​	未来避免某个属性值在训练集合没有与某个类同时出现导致概率值连乘为0，采用拉普拉斯修正



## 特征工程

​		特征工程，是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。 从本质上来讲，特征工程是一个表示和展现数据的过程。 在实际工作中，特征工程旨在去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系 

### 1特征选择

特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。

特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。

Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。

​		方差选择法：先计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。

​		相关系数法：计算各个特征对目标值的相关系数以及相关系数的P值。

​		卡方检验：检验定性自变量对定性因变量的相关性

​		互信息法：评价定性自变量对定性因变量的相关性

Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。

​			**递归特征消除法**：递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征

Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。

​			**基于惩罚项的特征选择法**：使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维

​			**基于树模型的特征选择法**：

# 集成学习

​	一般来说集成学习可以分为三大类：

- 用于减少方差的bagging（如Random Forest）
- 用于减少偏差的boosting（如AdaBoost）
- 用于提升预测结果的stacking

### 1.什么是XGBoost

​	XGBoost 是基于决策树的集成机器学习算法，它以梯度提升（Gradient Boost）为框架。XGBoost 用并行的方式实现了序列树的构建过程。XGBoost是由GBDT发展而来，同样是利用加法模型与前向分步算法实现学习的优化过程。时间复杂度：nlogn * d * m(n是样本个数，d是特征个数,m是树的深度)。**残差其实是最小均方损失函数关于预测值的反向梯度**

#### 	**1.1 xgboost怎么处理缺失值**和离散特征

​		xgboost把缺失值当做稀疏矩阵来对待，本身的在节点分裂时不考虑的缺失值的数值。缺失值数据会被分到左子树和右子树分别计算损失，选择较优的那一个。

​		**稀疏感知算法**，而为每个节点增加了一个缺省方向，当样本相应的特征值缺失时，可以被归类到缺省方向上，最优的缺省方向可以从数据中学到。至于如何学到缺省值的分支，其实很简单，分别枚举特征缺省的样本归为左右分支后的增益，选择增益最大的枚举项即为最优缺省方向。

​		无序特征：one-hot encoding，有序特征：利用神经网络的Embedding层处理类别特征

#### 	1.2为什么xgboost要二阶展开

​		二阶信息本身就能让梯度收敛更快更准确。这一点在优化算法里的牛顿法里已经证实了。可以简单认为一阶导指引梯度方向，二阶导指引梯度方向如何变化。这是从二阶导本身的性质。

**精准性**：相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数

**可扩展性**：损失函数支持自定义，只需要新的损失函数二阶可导。

#### 1.2.2xgb调参的参数，eta是什么

​		**learning rate**、每颗子树的最大深度、叶子节点的权重、正则化参数的系数、步长

#### 1.2.5XGBoost的并行

​		XGBoost的并行，指的是特征维度的并行：在训练之前，每个特征按特征值对样本进行预排序，并存储为Block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。

#### 1.2.6防止过拟合的方法

​	目标函数添加正则项：叶子节点个数+叶子节点权重的L2正则化。 减枝

​	列抽样：训练的时候只用一部分特征（不考虑剩余的block块即可）
​	子采样：每轮计算可以不使用全部样本，使算法更加保守
​	shrinkage: 可以叫学习率或步长，为了给后面的训练留出更多的学习空间

#### 1.2.7xgboost的分裂查找算法和缺点

1. Basic Exact Greedy Algorithm （精确贪心算法）

   ​	遍历所有特征，针对每个特征，把属于该节点的训练样本根据该特征值进行升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的分裂收益。选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置。分裂前后目标函数的变化。

2. Approximate Algorithm（近似算法）

​      精确贪心算法类似于CART中最优特征与切分点的查找，通过遍历每个特征下的每个可能的切分点取值，计算切分后的增益，选择增益最大的特征及切分点。

Global：学习每棵树前就提出候选切分点，并在每次分裂时都采用这种分割；

Local：每次分裂前将重新提出候选切分点

#### 1.2.8块结构设计

​	GBoost 在训练之前对根据特征对数据进行了排序，然后保存到块结构中，并在每个块结构中都采用了稀疏矩阵存储格式进行存储，后面的训练过程中会重复地使用块结构，可以大大减小计算量。

​	    每一个块结构包括一个或多个已经排序好的特征；缺失特征值将不进行排序；每个特征会存储指向样本梯度统计值的索引，方便计算一阶导和二阶导数值；

**缓存访问优化算法**

​	   块结构的设计可以减少节点分裂时的计算量，但特征值通过索引访问样本梯度统计值的设计会导致访问操作的内存空间不连续，这样会造成缓存命中率低，从而影响到算法的效率。

​        为了解决缓存命中率低的问题，XGBoost 提出了缓存访问优化算法：为每个线程分配一个连续的缓存区，将需要的梯度信息存放在缓冲区中，这样就是实现了非连续空间到连续空间的转换，提高了算法效率。

**缺点**

1. 虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集；
2. 预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存。

#### 1.2.9中的一棵树的停止生长条件

- 当新引入的一次分裂所带来的增益Gain<0时，放弃当前的分裂。这是训练损失和模型结构复杂度的博弈过程。
- 当树达到最大深度时，停止建树，因为树的深度太深容易出现过拟合，这里需要设置一个超参数max_depth。
- 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值，也会放弃此次分裂。这涉及到一个超参数:最小样本权重和，是指如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细。

### 1-3、LGB的提出

1. 单边梯度抽样算法；

   ​		GBDT 算法的梯度大小可以反应样本的权重，梯度越小说明模型拟合的越好，单边梯度抽样算法利用这一信息对样本进行抽样，减少了大量梯度小的样本，在接下来的计算中只需关注梯度高的样本，极大的减少了计算量。

   ​		GOSS 算法保留了梯度大的样本，并对梯度小的样本进行随机抽样，为了不改变样本的数据分布，在计算增益时为梯度小的样本引入一个常数进行平衡。**一方面算法将更多的注意力放在训练不足的样本上，另一方面通过乘上权重来防止采样对原始数据分布造成太大的影响。**

2. 直方图算法；

   **1) 直方图算法**

   ​		直方图算法的基本思想是将连续的特征离散化为 k 个离散特征，同时构造一个宽度为 k 的直方图用于统计信息（含有 k 个 bin）。利用直方图算法我们无需遍历数据，只需要遍历 k 个 bin 即可找到最佳分裂点。存储方便、运算更快、鲁棒性强、模型更加稳定。

   **2) 直方图加速**：

   通过父节点的直方图与相邻叶节点的直方图相减的方式构建，从而减少了一半的计算量。

   **3) 稀疏特征优化**：只用非零特征构建直方图。

3. 互斥特征捆绑算法；

   ​		高维特征往往是稀疏的，而且特征间可能是相互排斥的（如两个特征不同时取非零值），如果两个特征并不完全互斥（如只有一部分情况下是不同时取非零	值），可以用互斥率表示互斥程度。互斥特征捆绑算法指出如果将一些特征进行融合绑定，则可以降低特征数量

4. 基于最大深度的 Leaf-wise 的垂直生长算法；

   - Level-wise：基于层进行生长，直到达到停止条件；
   - Leaf-wise：每次分裂增益最大的叶子节点，直到达到停止条件。

   ​       XGBoost 采用 Level-wise 的增长策略，方便并行计算每一层的分裂节点，提高了训练速度，但同时也因为节点增益过小增加了很多不必要的分裂，降低了计算量；LightGBM 采用 Leaf-wise 的增长策略减少了计算量，配合最大深度的限制防止过拟合，由于每次都需要计算增益最大的节点，所以无法并行分裂

5. 类别特征最优分割；

   ​	    LightGBM 原生支持类别特征，采用 many-vs-many 的切分方式将类别特征分为两个子集，实现类别特征的最优切分。

6. 特征并行和数据并行；

   ​		LightGBM 采用优化后的特征并行、数据并行方法加速计算，当数据量非常大的时候还可以采用投票并行的策略

   ​		传统的特征并行算法在于对数据进行垂直划分，然后使用不同机器找到不同特征的最优分裂点，基于通信整合得到最佳划分点，然后基于通信告知其他机器划分结果。

   ​		传统的特征并行方法有个很大的缺点：需要告知每台机器最终划分结果，增加了额外的复杂度（因为对数据进行垂直划分，每台机器所含数据不同，划分结果需要通过通信告知）。

   ​		LightGBM 则不进行数据垂直划分，每台机器都有训练集完整数据，在得到最佳划分方案后可在本地执行划分而减少了不必要的通信。

   ​		LightGBM 采用分散规约（Reduce scatter）的方式将直方图整合的任务分摊到不同机器上，从而降低通信代价，并通过直方图做差进一步降低不同机器间的通信。

7. 缓存优化。

### 2.xgboost和gbdt的关系

​	    GBDT，它是一种基于boosting增强策略的加法模型，训练的时候采用前向分布算法进行贪婪的学习，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。

​	    XGBoost对GBDT进行了一系列优化，比如损失函数进行了二阶泰勒展开、目标函数加入正则项、支持并行和默认缺失值处理等，在可扩展性和训练速度上有了巨大的提升，但其核心思想没有大的变化

区别：

- **基分类器**：XGBoost的基分类器不仅支持CART决策树，还支持线性分类器，此时XGBoost相当于带L1和L2正则化项的Logistic回归（分类问题）或者线性回归（回归问题）。GBDT使用CART作为**基分类器**
- **导数信息**：XGBoost对损失函数做了二阶泰勒展开，GBDT只用了一阶导数信息，并且XGBoost还支持自定义损失函数，只要损失函数一阶、二阶可导。
- **正则项**：XGBoost的目标函数加了正则项， 相当于预剪枝，使得学习出来的模型更加不容易过拟合。
- **列抽样**：XGBoost支持列采样，与随机森林类似，用于防止过拟合。
- **缺失值处理**：对树中的每个非叶子结点，XGBoost可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。	传统GBDT没有专门针对**缺失值进行处理**
- **并行化**：注意不是tree维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度

#### 2.1随机森林、Adaboot与GBDT对比

​		随机森林：在数据集上表现良好，相对于其他算法有较大的优势，易于并行化，在大数据集上有很大的优势；能够处理高维度数据，不用做特征选择。在噪声较大的分类或者回归问题上会过拟合。对异常不敏感。

​		Adaboot： 优点：分类精度高；可以用各种回归分类模型来构建弱学习器，非常灵活；不容易发生过拟合。	缺点：对异常点敏感，异常点会获得较高权重。

​		**GBDT： 优点**，可以自动进行特征组合，拟合非线性数据；可以灵活处理各种类型的数据。 缺点，对异常点敏感。**随机森林不需要进行特征归一化。而GBDT则需要进行特征归一化**

GBDT 与 Adaboost 的对比：

**5.3.1 相同：**

1. 都是 Boosting 家族成员，使用弱分类器；都使用前向分布算法；

**5.3.2 不同：**

1. **迭代思路不同**：Adaboost 是通过提升错分数据点的权重来弥补模型的不足（利用错分样本），而 GBDT 是通过算梯度来弥补模型的不足（利用残差）
2. **损失函数不同**：AdaBoost 采用的是指数损失，GBDT 使用的是绝对损失或者 Huber 损失函数；

#### xgboost回归与分类



### 3.常见评价标准的优缺点

​	精确率、召回率。

​	真阳性率=TP/(TP+FN) 		假阳性率 = FP/(TN+FP)

3.1AUC（代码实现）

​		随机挑选一个正样本以及负样本，算法将正样本排在负样本前面的概率就是AUC值。 M为正类样本的数目，N为负类样本的数目。特点：**AUC的评价效果不受正负样本比例的影响。因为改变正负样本比例，横纵坐标大小同时变化。整体不变。**

​			假设分类器的输出是样本属于正类的socre（置信度），则AUC的物理意义为，任取一对（正、负）样本，正样本的score大于负样本的score的概率。

1.通过ROC曲线面积计算AUC

2.通过计算概率计算AUC

​			在有M个正样本,N个负样本的数据集里。一共有M*N对样本（一对样本即，一个正样本与一个负样本）。统计这M*N对样本里，正样本的预测概率大于负样本的预测概率的个数

​		接收器工作特性曲线（ROC）曲线下的面积—AUC是评估分类模型准确性的标准方法。它避免了在阈值选择过程中假定的主观性，当连续的概率得到的分数被转换为二分类标签时，通过总结整体模型表现，其衡量模型区分正负样本的性能优于通过阈值来判断的其他方法。

​		AUC反应了太过笼统的信息。无法反应召回率、精确率等在实际业务中经常关心的指标

​		对FPR和TPR两种错误的代价同等看待；这一点和第二点雷同，基本的意思就是当用户对不同类别的预测准确率有不同程度的需求时，auc不能很好的满足这个需求。

​		它没有给出模型误差的空间分布信息（我们不知道模型预测错误的具体情况，比如哪一类预测的错误多，比如整体错误的分布情况等等），AUC只关注正负样本之间的排序，并不关心正样本内部，或者负样本内部的排序，这样我们也无法衡量样本对于好坏客户的好坏程度的刻画能力；

### 4.常见的损失函数

​		**损失函数**用来评价模型的**预测值**和**真实值**不一样的程度，损失函数越好，通常模型的性能越好。不同的模型用的损失函数一般也不一样。

​		损失函数**分为**经验风险损失函数**和**结构风险损失函数**。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。

​		**1.0-1损失函数**：0-1损失是指预测值和目标值不相等为1， 否则为0:（感知器）

​		**2.绝对值损失函数**：绝对值损失函数是计算预测值与目标值的差的绝对值：

​		**3.对数似然损失函数**：

​		(1) log对数损失函数能非常好的表征概率分布，在很多场景尤其是多分类，如果需要知道结果属于每个类别的置信度，那它非常适合。

​		(2)健壮性不强，相比于hinge loss对噪声更敏感。

​		(3)**逻辑回归**的损失函数就是log对数损失函数。

​		**4.平方损失函数**

​	 	**5指数损失函数（exponential loss）**：对离群点、噪声非常敏感。经常用在AdaBoost算法中。

​		**6.交叉熵损失函数 (Cross-entropy loss function)**

​		(1)本质上也是一种**对数似然函数**，可用于二分类和多分类任务中

​		(2)当使用sigmoid作为激活函数的时候，常用**交叉熵损失函数**而不用**均方误差损失函数**，因为它可以**完美解决平方损失函数权重更新过慢**的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。

​		**7.Hinge Loss中文名叫合页损失函数**

​		hinge-loss的公式是：

![[公式]](https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5EN%5B1-y_i%28w%C2%B7x_i+%2B+b%29%5D_%2B+%2B+%5Clambda%7C%7Cw%7C%7C%5E2+%5C%5C+%5Bz%5D_%2B+%3D+%5Cbegin%7Bequation%7D+%5Cleft%5C%7B++++++++++++++%5Cbegin%7Barray%7D%7Blr%7D+++++++++++z%2C+z%3E0+%26++%5C%5C++++++++++++++0.z%5Cleq0+%26+++++++++++++++%5Cend%7Barray%7D++%5Cright.+%5Cend%7Bequation%7D+%5C%5C+)

​		第一项是损失，第二项是正则化项。这个公式就是说 ![[公式]](https://www.zhihu.com/equation?tex=y_i%28w%C2%B7x_i%2Bb%29) 大于1时loss为0， 否则loss为 ![[公式]](https://www.zhihu.com/equation?tex=1-y_i%28w%C2%B7x_i%2Bb%29) 。对比感知机的损失函数 ![[公式]](https://www.zhihu.com/equation?tex=%5B-y_i%28w%C2%B7x_i%2Bb%29%5D_%2B) 来说，hinge loss不仅要分类正确，而且置信度足够高的时候，损失才为0，对学习有更高的要求。

### 5.过拟合和样本不平衡的常见处理方式
训练数据集太小，过拟合出现的原因：

模型复杂度过高，参数过多
训练数据比较小
训练集和测试集分布不一致
样本里面的噪声数据干扰过大，导致模型过分记住了噪声特征，反而忽略了真实的输入输出特征
训练集和测试集特征分布不一样（如果训练集和测试集使用了不同类型的数据集会出现这种情况）
1、降低模型复杂度

　　处理过拟合的第一步就是降低模型复杂度。为了降低复杂度，我们可以简单地移除层或者减少神经元的数量使得网络规模变小。与此同时，计算神经网络中不同层的输入和输出维度也十分重要。虽然移除层的数量或神经网络的规模并无通用的规定，但如果你的神经网络发生了过拟合，就尝试缩小它的规模。

2、增加更多数据：使用更大的数据集训练模型

3、数据增强

　　使用数据增强可以生成多幅相似图像。这可以帮助我们增加数据集规模从而减少过拟合。因为随着数据量的增加，模型无法过拟合所有样本，因此不得不进行泛化。计算机视觉领域通常的做法有：翻转、平移、旋转、缩放、改变亮度、添加噪声等等，音频数据增强方法有：增加噪音、增加混响、时移、改变音调和时间拉伸

4、正则化
5、早停
7、重新清洗数据：把明显异常的数据剔除

8、使用集成学习方法：

​	L1、L2、神经网络里面的dropout、集成学习（bagging、boosting）、增加数据、数据均衡（过采样、降采样）、BN（加快训练、消除梯度消失和爆炸）

​		样本不平衡

- 欠采样：随机欠采样、easysampling、KNN 
- 过采样：随机过采样、SMOTE（人工合成） 
- 数据增强 
- 代价敏感学习：误分类代价不同
- 适合的评价指标：准确率、F值、AUC、G-Mean

### 6.深度学习优化方法总结

#### 	5.1:SGD（随机梯度下降）

​		SGD就是每一次迭代计算mini-batch的梯度，然后对参数进行更新，是最常见的优化方法了。即：

​																	![[公式]](https://www.zhihu.com/equation?tex=g_t%3D%5Cnabla_%7B%5Ctheta_%7Bt-1%7D%7D%7Bf%28%5Ctheta_%7Bt-1%7D%29%7D)

![[公式]](https://www.zhihu.com/equation?tex=%5CDelta%7B%5Ctheta_t%7D%3D-%5Ceta%2Ag_t)

​			其中，![[公式]](https://www.zhihu.com/equation?tex=%5Ceta)是学习率，![[公式]](https://www.zhihu.com/equation?tex=g_t)是梯度 SGD完全依赖于当前batch的梯度，所以![[公式]](https://www.zhihu.com/equation?tex=%5Ceta)可理解为允许当前batch的梯度多大程度影响参数更新

​		**缺点**

- 选择合适的learning rate比较困难，对所有的参数更新使用同样的learning rate。对于稀疏数据或者特征，有时我们可能想更快更新一些不经常出现的特征，对于常出现的特征更新慢一些，这时候SGD就不太能满足要求了

- SGD容易收敛到局部最优，并且在某些情况下可能被困在鞍点

  #### 5.2Momentum

  momentum是模拟物理里动量的概念，积累之前的动量来替代真正的梯度。公式如下：

  ​															![[公式]](https://www.zhihu.com/equation?tex=m_t%3D%5Cmu%2Am_%7Bt-1%7D%2Bg_t)

  ![[公式]](https://www.zhihu.com/equation?tex=%5CDelta%7B%5Ctheta_t%7D%3D-%5Ceta%2Am_t)

  **特点：**

  - 下降初期时，使用上一次参数更新，下降方向一致，乘上较大的![[公式]](https://www.zhihu.com/equation?tex=%5Cmu)能够进行很好的加速 
  - 下降中后期时，在局部最小值来回震荡的时候，![[公式]](https://www.zhihu.com/equation?tex=gradient%5Cto0)，![[公式]](https://www.zhihu.com/equation?tex=%5Cmu)使得更新幅度增大，跳出陷阱 
  - 在梯度改变方向的时候，![[公式]](https://www.zhihu.com/equation?tex=%5Cmu)能够减少更新 总而言之，momentum项能够在相关方向加速SGD，抑制振荡，从而加快收敛

  #### 5.3:Adagrad 

  Adagrad其实是对学习率进行了一个约束。即： 

  ![[公式]](https://www.zhihu.com/equation?tex=n_t%3Dn_%7Bt-1%7D%2Bg_t%5E2)

  ![[公式]](https://www.zhihu.com/equation?tex=%5CDelta%7B%5Ctheta_t%7D%3D-%5Cfrac%7B%5Ceta%7D%7B%5Csqrt%7Bn_t%2B%5Cepsilon%7D%7D%2Ag_t)

  此处，对![[公式]](https://www.zhihu.com/equation?tex=g_t)从1到![[公式]](https://www.zhihu.com/equation?tex=t)进行一个递推形成一个约束项regularizer，![[公式]](https://www.zhihu.com/equation?tex=-%5Cfrac%7B1%7D%7B%5Csqrt%7B%5Csum_%7Br%3D1%7D%5Et%28g_r%29%5E2%2B%5Cepsilon%7D%7D)，![[公式]](https://www.zhihu.com/equation?tex=%5Cepsilon)用来保证分母非0 

  **特点：**

  - 前期![[公式]](https://www.zhihu.com/equation?tex=g_t)较小的时候， regularizer较大，能够放大梯度 
  - 后期![[公式]](https://www.zhihu.com/equation?tex=g_t)较大的时候，regularizer较小，能够约束梯度 
  - 适合处理稀疏梯度 


  **缺点：**

  - 由公式可以看出，仍依赖于人工设置一个全局学习率 

  - ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta)设置过大的话，会使regularizer过于敏感，对梯度的调节太大 

  - 中后期，分母上梯度平方的累加将会越来越大，使![[公式]](https://www.zhihu.com/equation?tex=gradient%5Cto0)，使得训练提前结束

    

  #### 5.4 Adam

  	Adam(Adaptive Moment Estimation)本质上是带有动量项的RMSprop，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。公式如下： 

  ![[公式]](https://www.zhihu.com/equation?tex=m_t%3D%5Cmu%2Am_%7Bt-1%7D%2B%281-%5Cmu%29%2Ag_t)

  ![[公式]](https://www.zhihu.com/equation?tex=n_t%3D%5Cnu%2An_%7Bt-1%7D%2B%281-%5Cnu%29%2Ag_t%5E2)

  ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bm_t%7D%3D%5Cfrac%7Bm_t%7D%7B1-%5Cmu%5Et%7D)

  ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bn_t%7D%3D%5Cfrac%7Bn_t%7D%7B1-%5Cnu%5Et%7D)

  ![[公式]](https://www.zhihu.com/equation?tex=%5CDelta%7B%5Ctheta_t%7D%3D-%5Cfrac%7B%5Chat%7Bm_t%7D%7D%7B%5Csqrt%7B%5Chat%7Bn_t%7D%7D%2B%5Cepsilon%7D%2A%5Ceta)

  其中，![[公式]](https://www.zhihu.com/equation?tex=m_t)，![[公式]](https://www.zhihu.com/equation?tex=n_t)分别是对梯度的一阶矩估计和二阶矩估计，可以看作对期望![[公式]](https://www.zhihu.com/equation?tex=E%7Cg_t%7C)，![[公式]](https://www.zhihu.com/equation?tex=E%7Cg_t%5E2%7C)的估计；![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bm_t%7D)，![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bn_t%7D)是对![[公式]](https://www.zhihu.com/equation?tex=m_t)，![[公式]](https://www.zhihu.com/equation?tex=n_t)的校正，这样可以近似为对期望的无偏估计。 可以看出，直接对梯度的矩估计对内存没有额外的要求，而且可以根据梯度进行动态调整，而![[公式]](https://www.zhihu.com/equation?tex=-%5Cfrac%7B%5Chat%7Bm_t%7D%7D%7B%5Csqrt%7B%5Chat%7Bn_t%7D%7D%2B%5Cepsilon%7D)对学习率形成一个动态约束，而且有明确的范围。

  **特点：**

  - 结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点 
  - 对内存需求较小 
  - 为不同的参数计算不同的自适应学习率

  

### 7.*HMM*和*CRF*区别

1.HMM是生成模型，CRF是判别模型

2.HMM是概率有向图，CRF是概率无向图

3.HMM求解过程可能是局部最优，CRF可以全局最优

4.CRF概率归一化较合理，HMM则会导致label bias 问题

​		大的不同点是linear-CRF模型是判别模型，而HMM是生成模型，即linear-CRF模型要优化求解的是条件概率P(y|x),则HMM要求解的是联合分布P(x,y)。第二，linear-CRF是利用最大熵模型的思路去建立条件概率模型，对于观测序列并没有做马尔科夫假设。而HMM是在对观测序列做了马尔科夫假设的前提下建立联合分布的模型。

### 8、二分类交叉熵和多分类交叉熵的公式

### 9、熵、信息熵、交叉熵

​		熵表示信息的混乱程度，熵最小，这个时候最有秩序；而被打乱的时候，熵开始增大，直到最后一片混乱

​		一条信息的信息量大小和它的不确定性有很大的关系。一句话如果需要很多外部信息才能确定，我们就称这句话的信息量比较大。

​		信息量是对于单个事件来说的，但是实际情况一件事有很多种发生的可能，比如掷骰子有可能出现6种情况，明天的天气可能晴、多云或者下雨等等。**熵是表示随机变量不确定的度量，是对所有可能发生的事件产生的信息量的期望**。

​		信息熵主要是反应信息的不确定性，**在一个随机事件中，某个事件发生的不确定度越大，熵也就越大，那我们要搞清楚所需要的信息量越大。**

​		相对熵又称KL散度，用于衡量对于同一个随机变量x的两个分布p(x)和q(x)之间的差异。KL散度公式进行分解，前半部分就是p(x)的熵，后半部分就是我们的交叉熵

​		机器学习中，我们常常使用KL散度来评估predict和label之间的差别，但是由于KL散度的前半部分是一个常量，所以我们常常将后半部分的交叉熵作为损失函数，其实二者是一样的。

# 深度学习

### 	1.**Batch Normalization**

​		神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch 梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度。

​		而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布。使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。

​		**算法过程：**

- 沿着通道计算每个batch的均值u
- 沿着通道计算每个batch的方差σ^2
- 对x做归一化，x’=(x-u)/开根号(σ^2+ε)
- 加入缩放和平移变量γ和β ,归一化后的值，y=γx’+β
- 加入缩放平移变量的原因是：保证每一次数据经过归一化后还保留原有学习来的特征，同时又能完成归一化操作，加速训练。 这两个参数是用来学习的参数。

​		在训练的最后一个epoch时，要对这一epoch所有的训练样本的均值和标准差进行统计，这样在一张测试图片进来时，使用训练样本中的标准差的期望和均值的期对测试数据进行归一化

#### **Batch Normalization优点**与缺点

​	1）模型更稳定，训练速度加快。你可以选择比较大的初始学习率，让你的训练速度飙涨。以前还需要慢慢调整学习率，甚至在网络训练到一半的时候，还需要想着学习率进一步调小的比例选择多少比较合适，现在我们可以采用初始很大的学习率，然后学习率的衰减速度也很大，因为这个算法收敛很快。当然这个算法即使你选择了较小的学习率，也比以前的收敛速度快，因为它具有快速训练收敛的特性。

　　2）移除或使用较低的dropout，因为BN具有提高网络泛化能力的特性。

　　3）再也不需要使用使用局部响应归一化层了。

　　4）降低L2权重衰减系数。

- 对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布；
- BN实际使用时需要计算并且保存某一层神经网络batch的均值和方差等统计信息，对于对一个固定深度的前向神经网络（DNN，CNN）使用BN，很方便；但对于RNN来说，sequence的长度是不一致的，换句话说RNN的深度不是固定的，不同的time-step需要保存不同的statics特征，可能存在一个特殊sequence比其他sequence长很多，这样training时，计算很麻烦



​		LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；
​		BN中则针对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。

​		所以，LN不依赖于batch的大小和输入sequence的深度，因此可以用于batchsize为1和RNN中对边长的输入sequence的normalize操作


几种Normalization的区别https://blog.csdn.net/liuxiao214/article/details/81037416

将输入的图像shape记为[N, C, H, W]，这几个方法主要的区别就是在：

batchNorm是在batch上，对NHW做归一化，对小batchsize效果不好；
layerNorm在通道方向上，对CHW归一化，主要对RNN作用明显；
instanceNorm在图像像素上，对HW做归一化，用在风格化迁移；
GroupNorm将channel分组，然后再做归一化；
SwitchableNorm是将BN、LN、IN结合，赋予权重，让网络自己去学习归一化层应该使用什么方法。

​	为什么能缓解过拟合

​		在训练中，BN的使用使得一个mini-batch中的所有样本都被关联在了一起，因此网络不会从某一个训练样本中生成确定的结果。

​		这句话什么意思呢？意思就是同样一个样本的输出不再仅仅取决于样本本身，也取决于跟这个样本属于同一个mini-batch的其它样本。同一个样本跟不同的样本组成一个mini-batch，它们的输出是不同的（仅限于训练阶段，在inference阶段是没有这种情况的）。我把这个理解成一种数据增强：同样一个样本在超平面上被拉扯，每次拉扯的方向的大小均有不同。不同于数据增强的是，这种拉扯是贯穿数据流过神经网络的整个过程的，意味着神经网络每一层的输入都被数据增强处理了

LN中同层神经元输入拥有相同的均值和方差，不同的输入样本有不同的均值和方差；
BN中则针对不同神经元输入计算均值和方差，同一个batch中的输入拥有相同的均值和方差。

所以，LN不依赖于batch的大小和输入sequence的深度，因此可以用于batchsize为1和RNN中对边长的输入sequence的normalize操作。

### 3.CNN中使用BN和同步BN

​		在batch_size * channel * height * width这么大的一层中，对总共batch_size * height * width个像素点统计得到一个均值和一个标准差，共得到channel组参数。



### 4.dropout原理和优缺点

​		在机器学习的模型中，如果模型的参数太多，而训练样本又太少，训练出来的模型很容易产生过拟合的现象，Dropout可以比较有效的缓解过拟合的发生，在一定程度上达到正则化的效果。

​		在前向传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征

​	（1）首先随机（临时）删掉网络中一半的隐藏神经元，输入输出神经元保持不变

​	（2）然后把输入x通过修改后的网络前向传播，然后把得到的损失结果通过修改的网络反向传播。一小批训练样本执行完这个过程后，在没有被删除的神经元上按照随机梯度下降法更新对应的参数。

​		在测试模型阶段，每一个神经元都会用到。

​		**为什么说Dropout可以解决过拟合？**

​		**取平均的作用**dropout掉不同的隐藏神经元就类似在训练不同的网络，随机删掉一半隐藏神经元导致网络结构已经不同，整个dropout过程就相当于对很多个不同的神经网络取平均。而不同的网络产生不同的过拟合，一些互为“反向”的拟合相互抵消就可以达到整体上减少过拟合。

​		**减少神经元之间复杂的共适应关系：** 因为dropout程序导致两个神经元不一定每次都在一个dropout网络中出现。这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况 。迫使网络去学习更加鲁棒的特征 ，

​		预测模型的时候，每一个神经单元的权重参数要乘以概率p，以使得在训练时和测试时每一层输入有大致相同的期望。

缺点：训练时间长，

### 5.最小二乘法和极大似然估计的关系

​	最大似然估计：模型已知，参数未定。**利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值（模型已定，参数未知）**

​	最小二乘：找到一个（组）估计值，使得实际值与估计值的距离最小

​	      线性回归里面L2正则下的最小二乘法（欧式距离），就是高斯先验下的的最大后验估计。L1正则下（曼哈顿距离）的最小二乘法是拉普拉斯先验下的最大后验估。

​	    最大后验概率：：就是最大化在给定数据样本的情况下模型参数的后验概率。根据已知样本，来通过调整模型参数使得模型能够产生该数据样本的概率最大，只不过对于模型参数有了一个先验假设，即模型参数可能满足某种分布。



# 琐碎知识点

### 1.LR为什么不使用MSE作为损失函数和缺点

​		使用MSE作为损失函数的话，它的梯度是和sigmoid函数的导数有关的，如果当前模型的输出接近0或者1时，sigmoid函数的导数接近0。使得求得的梯度很小，损失函数收敛的很慢

​		缺点（特征相关情况）因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况，（特征空间）特征空间很大时，性能不好。（精度）容易欠拟合，精度不高。

### 2.LR为什么使用Sigmoid函数作为概率函数

​		需要一个单调可微的函数将分类任务的真实标记与线性回归模型的预测值联系起来。理想的事单位阶跃函数，不连续。sigmoid单调可微，且近视阶跃函数。

​		lr是基于伯努利分布为假设的，伯努利分布的指数形式就是sigmoid函数，而且sigmoid函数可以将数据压缩到0-1内，以便表示概率

### 3.L1损失与L2损失的异同点

**2.L1和L2的不同点**

>1.L1正则化可以产生稀疏权值矩阵，即产生一个稀疏模型，用于特征选择。所以L1适用于特征之间有关联的情况。 L1解空间为棱形，L2的解空间为圆。L1是先验部分加入拉普拉斯分布，拉普拉斯分布本身就是稀疏解
> 
>L2正则化可以产生参数值较小的模型，能适应不同的数据集，一定程度上防止过拟合，抗扰动能力强，L2比较平滑。 L2让所有特征的系数都缩小，但是不会减为0，它会使优化求解稳定快速。所以L2适用于特征之间没有关联的情况。
> 
>原因：L1每次更新的时候会更新一个定值，那么若干次迭代之后，权重就有可能减少为0。但是L2每个更新的时候更新的值的大小和w i 的值是有关系的。当w i 趋近与0时，那么对应的导数值也会更新，所以他会不停的接近0，但并不会是0。此外，我们还可以得到，L2相对L1更稳定一些。几何的角度，误差等高线和取值空间的交点可以看到L1更容易倾向一个权重偏大一个权重为0。L2更容易倾向权重都较小。

#### 缓解过拟合的本质原因

​		正则化降低了模型的拟合能力，使得模型结构变得更加简单。奥卡姆剃刀原则，“保证性能差别不大的情况下，越是简单的模型泛化性能越好”

​		把正则变成先验，L1正则化是给参数引入了标准拉普拉斯先验，L2正则化是给参数引入了标准高斯分布的先验，通过贝叶斯后验概率最大化（这个时候不是用极大似然估计来求解了）可以推导出带L1或L2正则项的损失函数的表达式，引入先验相当于对参数进行了约束，使得参数必须落在标准拉普拉斯分布或者标准高斯分布的范围中。



### 3.蓄水池抽样算法

​	从未知大小的n个数据里面抽取k个数据。构造为k的蓄水池，从第i个数据开始（i>k）,以k/i的概率决定要不要讲数据放入水池中去。若放，则随机将蓄水池里面的数据丢弃一个。



### 4.KMeans如何确定K值和优缺点，改进策略

​	最常用最简单的方法可视化数据，然后观察出聚类聚成几类比较合适

肘部法（误差平方和突然变小的k）、轮廓系数法。

​		**优点**：算法原理简单，处理快;当聚类密集时，类与类之间区别明显，效果好
​		**缺点**：K是事先给定的，K值选定难确定，对孤立点、噪声敏感，结果不一定是全局最优，只能保证局部最优。很难发现大小差别很大的簇及进行增量计算。结果不稳定，初始值选定对结果有一定的影响，计算量大。

​		**改进**：

​		样本过大：mini-batch，随机从整体当中做一个抽样，选取出一小部分数据来代替整体。

​		迭代次数过多：kmeans++，**簇是有向心性的**。也就是说在同一个簇附近的点都会被纳入这个簇的范围内，反过来说就是两个离得远的点属于不同簇的可能性比离得近的大。

​		首先随机1个簇中心，然后从下的n-1个点当中再随机出一个点来做下一个簇中心。设计一个机制，**使得距离所有簇中心越远的点被选中的概率越大，离得越近被随机到的概率越小**。（轮盘法）重复上述的过程，直到一共选出了K个簇中心为止。



### 5.随机森林和GBDT，通常哪个树的深度更深

​		随机森林的行采样(bagging)和列采样(feature bagging)都是为了减小模型之间的相关性使基学习器变得不同从而减小集成模型的方差，但这种随机性会导致随机森林的偏差有所增加（相比于单棵不随机树），因此随机森林的单棵树都会采用很深的决策树，并不进行剪枝操作，以减小每棵树的偏差，这使得每一棵决策树就是一个精通于某一个窄领域的专家。

​		RF的每棵决策树都最大可能的进行生长而不进行剪枝

### 6.常见的激活函数及优缺点

**为什么要用激活函数？，激活函数为什么是非线性的？**

​		如果使用线性激活函数，那么输入跟输出之间的关系为线性的，无论神经网络有多少层都是线性组合。**使用非线性激活函数是为了增加神经网络模型的非线性因素**，以便使网络更加强大，增加它的能力，使它可以学习复杂的事物，复杂的表单数据，以及表示输入输出之间非线性的复杂的任意函数映射。输出层可能会使用线性激活函数，但在**隐含层都使用非线性激活函数**。

**1.Sigmoid：在什么情况下适合使用 Sigmoid激活函数呢？**

- Sigmoid 函数的输出范围是 0 到 1。由于输出值限定在 0 到 1，因此它对每个神经元的输出进行了归一化；
- 用于将预测概率作为输出的模型。由于概率的取值范围是 0 到 1，因此 Sigmoid 函数非常合适；
- 求导容易

Sigmoid激活函数有哪些缺点？

- 倾向于梯度消失；
- 函数输出不是以 0 为中心的，这会降低权重更新的效率；
- Sigmoid 函数执行指数运算，计算机运行得较慢。

**2. Tanh / 双曲正切激活函数**：2/(1+e^(-2x)) -1

- tanh 的输出间隔为 1，并且整个函数以 0 为中心，比 sigmoid 函数更好；
- 在 tanh 图中，负输入将被强映射为负，而零输入被映射为接近零。

注意：在一般的二元分类问题中，tanh 函数用于隐藏层，而 sigmoid 函数用于输出层，但这并不是固定的，需要根据特定问题进行调整。

**3. ReLU激活函数**

优点：

- 防止梯度消失，形式简单，可以加快网络的训练速度
- 计算速度快得多。ReLU 函数中只存在线性关系，因此它的计算速度比 sigmoid 和 tanh 更快。

缺点：

1. Dead ReLU 问题。当输入为负时，ReLU 完全失效，但是在反向传播过程中，如果输入负数，则梯度将完全为零，sigmoid 函数和 tanh 函数也具有相同的问题；
2. 我们发现 ReLU 函数的输出为 0 或正数，这意味着 ReLU 函数不是以 0 为均值

**4. Softplus**

​		Softplus 函数类似于 ReLU 函数，但是相对较平滑，像 ReLU 一样是单侧抑制。它的接受范围很广：(0, + inf)。导数为Sigmoid函数

**5.Leaky ReLU**

​		激活函数的饱和：趋于负无穷和正无穷时，导数趋于0；sigmoid和双曲正切函数为饱和的激活函数。饱和的激活函数存在梯度消失现象。

### 7.生成模型与判别模型的区别

对于判别式模型来说求得P(Y|X)，对未见示例X，根据P(Y|X)可以求得标记Y。即可以直接判别出来，如上图的左边所示，实际是就是直接得到了判别边界。

而生成式模型求得P(Y,X)，对于未见示例X，你要求出X与不同标记之间的联合概率分布，然后大的获胜

### 8.分类问题损失函数采用交叉熵而不是MSE

​		损失函数求梯度，MSE的梯度与sigmoid的导数有关，导数sigmoid ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%5E%7B%27%7D%28z%29+) 在输出接近 0 和 1 的时候是非常小的，故导致在使用最小均方差Loss时，模型参数w会学习的非常慢。而使用交叉熵Loss则没有这个问题。为了更快的学习速度，分类问题一般采用交叉熵损失函数。

### 9.当数据量特别大的时候，逻辑回归(LR)怎么做并行化处理？

### 10.常见的距离公式

​		假设我们有两个向量 ![[公式]](https://www.zhihu.com/equation?tex=X%3D%5BX_1%2C+...X_n%5D) 和 ![[公式]](https://www.zhihu.com/equation?tex=Y%3D%5BY_1%2C+...Y_n%5D) ，长度均为 ![[公式]](https://www.zhihu.com/equation?tex=n) 。

​		**欧氏距离（Euclidean Distance）**是常见的相似性度量方法，可求两个向量间的距离，取值范围为0至正无穷。欧氏距离计算默认对于每一个维度给予相同的权重，因此如果不同维度的取值范围差别很大，那么结果很容易被某个维度所决定。解决方法除了对数据进行处理以外，还可以使用加权欧氏距离，不同维度使用不同的权重。本文中我们使用的是欧氏距离的平方。

- **公式1**： ![[公式]](https://www.zhihu.com/equation?tex=d%28X%2CY%29%3D%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%28X_n-Y_n%29%5E2%7D)

**Pearson相关性系数（Pearson Correlation）**是衡量向量相似度的一种方法。输出范围为-1到+1, 0代表无相关性，负值为负相关，正值为正相关。皮尔森相关系数有一个明显的缺陷就是，它只对线性关系敏感。如果关系是非线性的，哪怕两个变量之间是一一对应的关系，皮尔森相关系数也可能接近0.

- **公式2**： ![[公式]](https://www.zhihu.com/equation?tex=%5Crho%28X%2CY%29%3D%5Cfrac%7BE%5B%28X-%5Cmu_%7BX%7D%29%28Y-%5Cmu_%7BY%7D%29%5D%7D%7B%5Csigma_X%5Csigma_Y%7D+%3D%5Cfrac%7BE%5B%28X-%5Cmu_%7BX%7D%29%28Y-%5Cmu_%7BY%7D%29%5D%7D%7B%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%28X_i-%5Cmu_X%29%5E2%7D%7D%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%28Y_i-%5Cmu_Y%29%5E2%7D%7D%7D)

**Cosine相似度也是一种相似性度量**，输出范围和Pearson相关性系数一致，含义也相似。

- **公式3**： ![[公式]](https://www.zhihu.com/equation?tex=c%28X%2CY%29%3D%5Cfrac%7BX%5Ccdot+Y%7D%7B%5Cleft%7C+X+%5Cright%7C%5Cleft%7C+Y+%5Cright%7C%7D+%3D%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7BX_iY_i%7D%7D%7B%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7BX_i%5E2%7D%7D%5Csqrt%7B%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7BY_i%5E2%7D%7D%7D)

### 11.偏差和方差的区别

​	**Bias**是用**所有可能的训练数据集**训练出的**所有模型**的输出的**平均值**与**真实模型**的输出值之间的差异。

误差，对象是单个模型，期望输出与真实标记的差别（可以解释为描述了 模型对本训练集的拟合程度） 

​	**Variance**是**不同的训练数据集训练出的模型**输出值之间的差异，方差，对象是多个模型，训练结果的分散程度

### 12.机器学习的分类

​	一般可以分为监督学习、无监督学习、半监督学习

​	按照模型分类：概率模型与非概率模型、线性模型与非线性模型、参数化模型与非参数化模型

### 13交叉熵损失与均方差损失的区别

​		分类任务上常用的激活函数是sigmoid，如果使用均方误差的话，在使用梯度下降算法更新时，权值w的偏导会含有sigmoid函数导数项(在输出接近0和1时会非常小)，导致训练阶段学习速度会变得很慢，而如果用交叉熵的话，权值w的偏导时不含sigmoid函数的导数项的(可以自己推导一下)，所以不会出现这个问题。所以在分类任务上，我们一般使用交叉熵

​		mse是非凸函数，有多个极值点，不适用做损失函数	。
机器学习中，我们常常使用KL散度来评估predict和label之间的差别，但是由于KL散度的前半部分是一个常量，所以我们常常将后半部分的交叉熵作为损失函数，其实二者是一样的。

​		判定方法可利用定义法、已知结论法以及函数的[二阶导数](https://baike.baidu.com/item/二阶导数)，对于实数集上的凸函数，一般的判别方法是求它的二阶导数，如果其二阶导数在区间上小于等于零，就称为凸函数。如果其二阶导数在区间上恒小于0，就称为严格凸函数	

### 14.大数定律

​		这个定理就是，在试验不变的条件下，重复试验多次，随机事件的频率近似于它的概率。

### 15先验概率、后验概率与似然函数

​		这种先知道结果，然后由结果估计原因的概率分布，*p*(交通方式|时间)，就是后验概率。

​		先于结果，确定原因的概率分布，*p*(交通方式)，就是先验概率。

​		这种先确定原因，根据原因来估计结果的概率分布，*p*(时间|交通方式)，就是似然估计。

### 16、连续数据离散化的好处

	1. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
	2. 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰
	3.  特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险
	4. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合

### 17、二分类、多分类和多标签分类的损失函数



### 18、调参的超参数

​	学习率、epoch 迭代次数、隐藏层、激活函数、batch size、网络层数，优化器，如：Adam的β1和β2参数，SGD……

### 19、计算softmax时的溢出问题

​		softmax(x)=softmax(x+c)也就是证明加了偏移c之后，对整个softmax层的作用不起影响

### 20、泛化误差如何产生，有哪些方法可以减小

​		即真实情况下模型的误差。**训练数据集样本单一，样本不足**，**训练数据中噪声干扰过大**，**模型过于复杂**。

​		增加数据的的多样性（数据增强）；简化模型，提高泛化能力；降低特征数量，删除冗余特征；dropout、提取终止训练

### 21、衡量分布之间的差异

​		KL散度、F-散度、

### 22、哪些机器学习需要归一化

​		机器学习中需要归一化的算法有SVM,逻辑回归，神经网络，KNN,线性回归，而**树形结构的不需要归一化**，因为它们不关心变量的值，而是关心变量分布和变量之间的条件概率，如决策树，随机森林，对于树形结构，树模型的构造是通过寻找最优分裂点构成的，样本点的数值缩放不影响分裂点的位置，对树模型的结构不造成影响，而且树模型不能进行梯度下降，因为树模型是阶跃的，阶跃是不可导的，并且求导没意义，也不需要归一化。

​		对于那些需要归一化的模型，主要是因为特征值相差很大时，运用梯度下降，损失等高线是椭圆形，需要进行多次迭代才能达到最优点，如果进行归一化了，那么等高线就是圆形的，促使SGD往原点迭代，从而导致需要迭代次数较少。

### 23、缺失值的处理策略

​		 **平均值填充**或者众数填充、**最近邻法**，先根据欧式距离或相关分析来确定距离具有缺失数据样本最近的K个样本，将这K个值加权平均来估计该样本的缺失数据。**热卡填充**，对于一个包含空值的对象，热卡填充法在完整数据中找到一个与它最相似的对象，然后用这个相似对象的值来进行填充。

### 24、贝叶斯公式与全概率公式

​		P(A|B)=P(B|A)*P(A)/P(B)

​		**后验概率（新信息出现后的A概率）　＝　先验概率（A概率） ｘ 可能性函数（新信息带来的调整）**

​		 **贝叶斯公式就是当已知结果，问导致这个结果的第i原因的可能性是多少？执果索因！**

​	    全概率就是表示达到某个目的，有多种方式（或者造成某种结果，有多种原因），问达到目的的概率是多少（造成这种结果的概率是多少）？



# 深度学习细节

### 	1.为什神经网络不能全初始化为0

​		因为如果网络中的每个神经元计算相同的输出，那么它们在反向传播过程中也会计算相同的梯度，并经历完全相同的参数更新。就是如果神经元的权值被初始化为相同的，那么神经元之间就没有不对称的根源。由于权重的对称性，隐层的神经单元输出始终不变，出现隐藏神经元的对称性。我们希望不同神经元能够有不同的输出，这样的神经网络才有意义。

### 	2.训练时loss变大是什么原因导致的

​		根本原因还是学习率太大，建议用指数衰减的学习率或者直接调用Rmsprop，Adam等现成的优化算法。当梯度下降到了loss function的梯度陡峭的地方时，梯度下降的步长会变的很大，很有可能跨过loss function的谷底，而另一面的梯度如果也很大，则会导致步长越来越大，导致优化步骤不能够收敛。

### 	3.样本不平衡的问题和解决方案

  		样本分布不均衡将导致样本量少的分类所包含的特征过少，并很难从中提取规律；即使得到分类模型，也容易产生过度依赖于有限的数据样本而导致过拟合的问题，当模型应用到新的数据上时，模型的准确性和鲁棒性将很差。	

​		欠采样：**在少量样本数量不影响模型训练的情况下**，可通过对**多数类样本欠采样**，实现少数样本和多数样本均衡。随机法：随机的删除一些多数类样本，使少数类样本和多数类样本数量达到均衡。

​		过采样：在少量样本数量**不支持**模型训练的情况下，可以通过对**少数类样本过采样**，实现少数样本和多数样本的均衡。

​		模型算法：通过引入有权重的模型算法，**针对少量样本着重拟合**，以提升对少量样本特征的学习。

### 	4.池化层的作用以及池化层的方向传播

​	   池化层没有参数，池化（pooling） 的本质，其实就是采样。Pooling 对于输入的 Feature Map，选择某种方式对其进行降维压缩，以加快运算速度。特征选择，信息过滤的过程。

​		**池化的作用：**

（1）保留主要特征的同时减少参数和计算量，防止过拟合。增加非线性、防止过拟合及提升模型泛化能力

（2）invariance(不变性)，这种不变性包括translation(平移)，rotation(旋转)，scale(尺度)。

​		池化层在反向传播时，它是不可导的，因为它是对特征图进行下采样会导致特征图变小。在反向传播时，梯度是按位传播的，那么，一个解决方法，就是如何构造按位的问题，**但一定要遵守传播梯度总和保持不变的原则**。

​		对于平均池化，其前向传播是取某特征区域的平均值进行输出，在反向传播时，框架需要将梯度平均分配给每一个神经元再进行反向传播。当每个像素都有用，提取全局特征的时候用平均池化。

​		对于最大池化，其前向传播是取某特征区域的最大值进行输出，这个区域仅有最大值神经元参与了前向传播，因此，在反向传播时，框架仅需要将该区域的梯度直接分配到最大值神经元。在网络低层，过滤掉一些信息时候，用最大池化层。

### 	5.1*1卷积的作用

​		1x1卷积实际上是对每个像素点，在不同的channels上进行线性组合（信息整合），且保留了图片的原有平面结构，调控depth，从而完成升维或降维的功能。

​		1x1卷积+relu函数可以增加网络的非线性，在不增加或者减少网络的参数的情况下，提升网络拟合能力

### 6.LSTM梯度消失与爆炸的原因和解决方法

​		在反向传播过程中，根据链式连乘法则，会多次出现对激活函数的求导，如果此部分大于1，那么层数增多的时候，最终的求出的梯度更新将以指数形式增加，即发生**梯度爆炸**，如果此部分小于1，那么随着层数增多，求出的梯度更新信息将会以指数形式衰减，即发生了**梯度消失**。

​		反向传播过程中，前面网络权重的偏导数的计算是逐渐从后往前累乘的，如果使用 ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%E3%80%81%5Ctanh)激活函数的话，由于导数小于一，因此累乘会逐渐变小，导致梯度消失，前面的网络层权重更新变慢；如果权重 ![[公式]](https://www.zhihu.com/equation?tex=w) 本身比较大，累乘会导致前面网络的参数偏导数变大，产生数值上溢。

​		因为 sigmoid 导数最大为1/4，故只有当abs(w)>4时才可能出现梯度爆炸，因此最普遍发生的是梯度消失问题。

​		两种情况下梯度消失经常出现，一是在**深层网络**中，二是采用了**不合适的损失函数**，比如sigmoid。梯度爆炸一般出现在深层网络和**权值初始化值太大**的情况下，下面分别从这两个角度分析梯度消失和爆炸的原因。

​	解决方法：预训练和微调；**梯度剪切**和正则化；使用relu、leakrelu等激活函数；BN；残差结构。

​	    relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。`解决了梯度消失、爆炸的问题`,`计算方便，计算速度快加速了网络的训练`

同时也存在一些**缺点**：

```
由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决）
输出不是以0为中心的
```



### 	.dataloader里的参数

dataset，这个就是PyTorch已有的数据读取接口（比如torchvision.datasets.ImageFolder）或者自定义的数据接口的输出，该输出要么是torch.utils.data.Dataset类的对象，要么是继承自torch.utils.data.Dataset类的自定义类的对象。

batch_size，根据具体情况设置即可。

shuffle，一般在训练数据中会采用。

collate_fn，是用来处理不同情况下的输入dataset的封装，一般采用默认即可，除非你自定义的数据读取输出非常少见。

batch_sampler，从注释可以看出，其和batch_size、shuffle等参数是互斥的，一般采用默认。

sampler，从代码可以看出，其和shuffle是互斥的，一般默认即可。

num_workers，从注释可以看出这个参数必须大于等于0，0的话表示数据导入在主进程中进行，其他大于0的数表示通过多个进程来导入数据，可以加快数据导入速度。

pin_memory，注释写得很清楚了： pin_memory (bool, optional): If True, the data loader will copy tensors into CUDA pinned memory before returning them. 也就是一个数据拷贝的问题。

timeout，是用来设置数据读取的超时时间的，但超过这个时间还没读取到数据的话就会报错。

### 7、数据不平衡长尾分布如何处理

1. **重采样**（re-sampling）：更具体可分为对少样本的**过采样**[[3\]](#ref_3)，或是对多样本的**欠采样**[[8\]](#ref_8)。但因过采样容易overfit到minor class，无法学到更鲁棒易泛化的特征，往往在非常不平衡数据上表现会更差；而欠采样则会造成major class严重的信息损失，导致欠拟合发生。
2. **数据合成**（synthetic samples）：即生成和少样本相似的“新”数据。经典方法SMOTE[[9\]](#ref_9)，思路简单来讲是对任意选取的少类样本，用K近邻选取其相似样本，通过对样本线性插值得到新样本。这里会想到和mixup[[10\]](#ref_10)很相似，于是也有imbalance的mixup版本出现[[11\]](#ref_11)。
3. **重加权**（re-weighting）：对不同类别（甚至不同样本）分配不同权重。注意这里的权重可以是自适应的。此类方法的变种有很多，有最简单的按照类别数目的倒数来做加权[[12\]](#ref_12)，按照“有效”样本数加权[[1\]](#ref_1)，根据样本数优化分类间距的loss加权[[4\]](#ref_4)，等等。
4. **迁移学习**（transfer learning）：这类方法的基本思路是对多类样本和少类样本分别建模，将学到的多类样本的信息/表示/知识迁移给少类别使用。代表性文章有[[13\]](#ref_13)[[14\]](#ref_14)。
5. **度量学习**（metric learning）：本质上是希望能够学到更好的embedding，对少类附近的boundary/margin更好的建模。有兴趣的同学可以看看[[15\]](#ref_15)[[16\]](#ref_16)。
6. **元学习/域自适应**（meta learning/domain adaptation）：分别对头部和尾部的数据进行不同处理，可以去自适应的学习如何重加权[[17\]](#ref_17)，或是formulate成域自适应问题[[18\]](#ref_18)。
7. **解耦特征和分类器**（decoupling representation & classifier）：最近的研究发现将特征学习和分类器学习解耦，把不平衡学习分为两个阶段，在特征学习阶段正常采样，在分类器学习阶段平衡采样，可以带来更好的长尾学习结果[[5\]](#ref_5)[[6\]](#ref_6)。这也是目前的最优长尾分类算法。

### 8、损失函数的重要性

​		**在进行神经网络的学习时，不能将识别精度作为指标，因为如果以识别精度为指标，导数大多数情况下都为0**。**损失函数一定要是连续的**

### 9、感受野的理解

​		在卷积神经网络中，感受野的定义是卷积神经网络每一层输出的特征图（feature map）上的像素点在**原始图像**上映射的区域大小。每个特征图上的神经元可以“感知”到的原图的大小。

​		一般task要求感受野越大越好，如图像分类中最后卷积层的感受野要大于输入图像，网络深度越深感受野越大性能越好。

​		感受野的计算公式如下： lk=lk−1+[(fk−1)∗(s1,sk-1)]

其中 lk−1为第 k−1 层对应的感受野大小， fk为第 k层的卷积核大小，或者是池化层的池化尺寸大小

### 10、深度学习模型压缩

#### 	1.矩阵分解

​		**将M\*N的矩阵分解为M\*K + K\*N，只要让K<<M 且 K << N，就可以大大降低模型体积。**

#### 	2.分组卷积

​	 3.分解卷积

​		使用两个串联小卷积核来代替一个大卷积核，两个3x3的卷积核代替一个5x5的卷积核；使用两个并联的非对称卷积核来代替一个正常卷积核，将一个7x7的卷积拆分成了一个1x7和一个7x1, 卷积效果相同的情况下，大大减少了参数量，同时还提高了卷积的多样性。

​	4.其他

​	平均池化层代替全连接；1*1的卷积，

​	5.模型剪枝

​		剪枝主要分为结构化剪枝和非结构化剪枝．结构化剪枝主要是通道剪枝、层剪枝等；非结构化剪枝也即权重剪枝。训练的神经网络中以较小的权重对值贡献较小，代表的是不重要的连接，因此可以个修剪掉。

### 11、 Relu在零点不可导，那么在反向传播中怎么处理？

​		caffe间断点的求导按左导数来计算。也就是默认情况下（negative_slope=0）,间断点处的导数认为是0.

### 12、神经网络权重初始化方法

​		 在深度学习中，神经网络的权重初始化方法对（weight initialization）对模型的收敛速度和性能有着至关重要的影响。说白了，神经网络其实就是对权重参数w的不停迭代更新，以期达到较好的性能。在深度神经网络中，随着层数的增多，我们在梯度下降的过程中，极易出现梯度消失或者梯度爆炸。因此，对权重w的初始化则显得至关重要，一个好的权重初始化虽然不能完全解决梯度消失和梯度爆炸的问题，但是对于处理这两个问题是有很大的帮助的，并且十分有利于模型性能和收敛速度。在这篇博客中，我们主要讨论四种权重初始化方法

​		**1、全零初始化和随机初始化**

​		如果神经元的权重被初始化为0， 在第一次更新的时候，除了输出之外，所有的中间层的节点的值都为零。一般神经网络拥有对称的结构，那么在进行第一次误差反向传播时，更新后的网络参数将会相同，在下一次更新时，相同的网络参数学习提取不到有用的特征，因此深度学习模型都不会使用0初始化所有参数。

​	**2.标准初始化**
相同的标准正态分布初始化层权值
​	3.xavier
Xavier初始化将一个层的权重设置为从一个有界的随机均匀分布中选择的值。，Xavier权值初始化将保持激活和反向传播梯度的方差，一直向上或向下传播到网络层。
4、Kaiming初始化（he初始化）
为给定层上的权值矩阵创建一个张量，并用从标准正态分布中随机选择的数字填充它。
将每个随机选择的数字乘以√2/√n，其中n是从上一层的输出(也称为“扇入”)进入给定层的连接数。
偏置张量初始化为零。
为了使得网络中信息更好的流动，每一层输出的方差应该尽量相等。



### 13、卷积层的时间和空间复杂度

​		![[公式]](https://www.zhihu.com/equation?tex=%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Ctextbf%7BTime%7D+%5Csim+O%28M%5E2+%5Ccdot+K%5E2+%5Ccdot+C_%7Bin%7D+%5Ccdot+C_%7Bout%7D%29%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad)

- ![[公式]](https://www.zhihu.com/equation?tex=M) 每个卷积核**输出**特征图 ![[公式]](https://www.zhihu.com/equation?tex=%28Feature~%5Ccolor%7Bred%7D%7BM%7Dap%29) 的边长
- ![[公式]](https://www.zhihu.com/equation?tex=K) 每个卷积核 ![[公式]](https://www.zhihu.com/equation?tex=%28%5Ccolor%7Bred%7D%7BK%7Dernel%29) 的边长
- ![[公式]](https://www.zhihu.com/equation?tex=C_%7Bin%7D) 每个卷积核的通道数，也即输入通道数，也即上一层的输出通道数。
- ![[公式]](https://www.zhihu.com/equation?tex=C_%7Bout%7D) 本卷积层具有的卷积核个数，也即输出通道数。

- **输出**特征图尺寸本身又由**输入**矩阵尺寸 ![[公式]](https://www.zhihu.com/equation?tex=X) 、卷积核尺寸 ![[公式]](https://www.zhihu.com/equation?tex=K) 、![[公式]](https://www.zhihu.com/equation?tex=Padding)、 ![[公式]](https://www.zhihu.com/equation?tex=Stride) 这四个参数所决定，表示如下：

![[公式]](https://www.zhihu.com/equation?tex=%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad+M+%3D+%28X+-+K+%2B+2+%2A+Padding%29+%2F+Stride+%2B+1+%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad)

空间复杂度（访存量），严格来讲包括两部分：总参数量 + 各层输出特征图。

- **参数量**：模型所有带参数的层的权重参数总量（即**模型体积**，下式第一个求和表达式）
- **特征图**：模型在实时运行过程中每层所计算出的输出特征图大小（下式第二个求和表达式）

![[公式]](https://www.zhihu.com/equation?tex=%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad+%5Ctextbf%7BSpace%7D+%5Csim+O%5CBigg%28%5Csum_%7Bl%3D1%7D%5E%7BD%7D+K_l%5E2+%5Ccdot+C_%7Bl-1%7D+%5Ccdot+C_%7Bl%7D+%2B+%5Csum_%7Bl%3D1%7D%5E%7BD%7D+M%5E2+%5Ccdot+C_l+%5CBigg%29+%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad%5Cquad)

- 总参数量只与卷积核的尺寸 ![[公式]](https://www.zhihu.com/equation?tex=K) 、通道数 ![[公式]](https://www.zhihu.com/equation?tex=C) 、层数 ![[公式]](https://www.zhihu.com/equation?tex=D) 相关，而**与输入数据的大小无关**。
- 输出特征图的空间占用比较容易，就是其空间尺寸 ![[公式]](https://www.zhihu.com/equation?tex=M%5E2) 和通道数 ![[公式]](https://www.zhihu.com/equation?tex=C) 的连乘。

### 14、Depthwise*卷积*与Pointwise*卷积*

### 15、pytorch和tensorflow的区别

​		1.实现方式：符号式编程vs命令式编程

​		tensorflow是纯符号式编程，而pytorch是命令式编程。命令式编程优点是实现方便，缺点是运行效率低。符号式编程通常是在计算流程完全定义好后才被执行，因此效率更高，但缺点是实现复杂。

​		2.图的定义：动态定义vs静态定义

​		两个框架都是在张量上进行运算，但是却存在着很大的差别。TensorFlow遵循"数据即代码，代码即数据"的理念，可以在运行之前静态的定义图，然后调用session来执行图。

​		pytorch中图的定义是动态化的，可以随时定义、随时更改、随时执行节点。因此相对而言，pytorch更加灵活，更加方便调试。

​		3.可视化：tensorboard vs nothing

​		TensorFlow最吸引人的地方之一就是tensorboard，可以清晰的看出计算图、网络架构，而pytorch自己没有类似tensorboard的工具，但是pytorch可以导入tensorboardx或者matplotlib这类工具包用于数据可视化。

### 16、LSTM与GRU
1. 忘记阶段。这个阶段主要是对上一个节点传进来的输入进行选择性忘记。简单来说就是会 “忘记不重要的，记住重要的”。

具体来说是通过计算得到的 [公式] （f表示forget）来作为忘记门控，来控制上一个状态的 [公式] 哪些需要留哪些需要忘。

2. 选择记忆阶段。这个阶段将这个阶段的输入有选择性地进行“记忆”。主要是会对输入 [公式] 进行选择记忆。哪些重要则着重记录下来，哪些不重要，则少记一些。当前的输入内容由前面计算得到的 [公式] 表示。而选择的门控信号则是由 [公式] （i代表information）来进行控制。

将上面两步得到的结果相加，即可得到传输给下一个状态的 [公式] 。也就是上图中的第一个公式。
3. 输出阶段。这个阶段将决定哪些将会被当成当前状态的输出。主要是通过 [公式] 来进行控制的。并且还对上一阶段得到的 [公式] 进行了放缩（通过一个tanh激活函数进行变化）

​		在LSTM中，引入了三个门函数：输入门、遗忘门和输出门。而在GRU中，只有两个门：更新门和重置门

​		sigmoid 用在了各种gate上，产生0~1之间的值，这个一般只有sigmoid最直接了。

​		tanh 用在了状态和输出上，是对数据的处理，这个用其他激活函数或许也可以。

​		sigmoid的输出在0-1之同，符合门控的物理定义，且当输入较大或较小时，其输出会非常接近1或0，从而保证该门开或关，在生成候选记亿时，使用**tanh函数，是因为其输出在-1-1之间，这与大多数场景下特征分布是0中心的吻合**。此外，**tanh函数在输入为0近相比 Sigmoid函数有更大的梯度，通常使模型收敛更快。**

​	重置门r：用于控制前一时刻的隐含层状态有多大程度更新到当前候选隐含层状态

​	更新门up：用于控制前一时刻的隐含层状态有多大程度更新到当前隐含层状态

​			GRU的前向传播公式：

![[公式]](https://www.zhihu.com/equation?tex=r_t%3D%5Csigma%28W_rx_t%2BU_rh_%7Bt-1%7D%2Bb_r%29+%5C%5C)![[公式]](https://www.zhihu.com/equation?tex=z_t%3D%5Csigma%28W_zx_t%2BU_zh_%7Bt-1%7D%2Bb_z%29+%5C%5C)![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7Bh%7D_t%3Dtanh%28W_hx_t%2BU_h%28r_t%5Codot+h_%7Bt-1%7D%29%2Bb_h%29+%5C%5C)![[公式]](https://www.zhihu.com/equation?tex=h_t%3Dz_t+%5Codot+h_%7Bt-1%7D+%2B+%281-+z_t%29+%5Codot+%5Chat%7Bh%7D_%7Bt%7D+%5C%5C)

### 17、卷积类型

### 18、Resnet的动机

​		从经验来看，网络的深度对模型的性能至关重要，当增加网络层数后，网络可以进行更加复杂的特征模式的提取，实验发现深度网络出现了退化问题：网络深度增加时，网络准确度出现饱和，甚至出现下降。（排除了过拟合和梯度消失）

​		由于非线性激活函数Relu的存在，每次输入到输出的过程都几乎是不可逆的，有了信息损失，让模型的内部结构至少有恒等映射的能力。以保证在堆叠网络的过程中，网络至少不会因为继续堆叠而产生退化。如果深层网络后面的层都是是**恒等映射**，那么模型就可以转化为一个浅层网络。那现在的问题就是**如何得到恒等映射**了。

​	但如果把网络设计为**H(x) = F(x) + x，即直接把恒等映射作为网络的一部分**。就可以把问题转化为**学习一个残差函数F(x) = H(x) - x.**

​		只要F(x)=0，就构成了一个恒等映射H(x) = x。 而且，拟合残差至少比拟合恒等映射容易得多。

​		通过跳接在**激活函数前，**将上一层（或几层）**之前的输出与本层**计算的**输出相加**，将求和的结果输入到激活函数中做为本层的输出。

### 19、U-Net上采样方法

- **插值**的方法，比如最近邻插值，对每个像素复制两遍；更复杂一点的还有双线性插值、立方插值等。
- **Transposed convolution**的方法（反卷积） 2x2 卷积进行上采样，

### 20、DenseNet

​		DenseNet模型，它的基本思路与ResNet一致，但是它建立的是前面所有层与后面层的密集连接（dense connection），它的名称也是由此而来。DenseNet的另一大特色是通过特征在channel上的连接来实现特征重用（feature reuse）。这些特点让DenseNet在参数和计算成本更少的情形下实现比ResNet更优的性能

​		相比ResNet，DenseNet提出了一个更激进的密集连接机制：即互相连接所有的层，具体来说就是每个层都会接受其前面所有层作为其额外的输入。图1为ResNet网络的连接机制，作为对比，图2为DenseNet的密集连接机制。可以看到，ResNet是每个层与前面的某层（一般是2~3层）短路连接在一起，连接方式是通过元素级相加。而在DenseNet中，每个层都会与前面所有层在channel维度上连接（concat）在一起（这里各个层的特征图大小是相同的，后面会有说明），并作为下一层的输入。对于一个 ![[公式]](https://www.zhihu.com/equation?tex=L) 层的网络，DenseNet共包含 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7BL%28L%2B1%29%7D%7B2%7D) 个连接，相比ResNet，这是一种密集连接。而且DenseNet是直接concat来自不同层的特征图，这可以实现特征重用，提升效率，这一特点是DenseNet与ResNet最主要的区别

### 21、pytorch train和eval的区别

​		dropout在训练的时候起作用， 在推断的时候被绕过不起作用， 或者等价地，将其概率置为零。 batch normalization 也和dropout一样， 有两种模式，分别对应训练和推断，分别是: model.eval()和model.train().

### 22、归一化BN、LN的区别
	 Batch Normalization：

   1.BN的计算就是把每个通道的NHW单独拿出来归一化处理

   2.针对每个channel我们都有一组γ,β，所以可学习的参数为2*C

   3.当batch size越小，BN的表现效果也越不好，因为计算过程中所得到的均值和方差不能代表全局
	Layer Normalizaiton：

   1.LN的计算就是把每个CHW单独拿出来归一化处理，不受batchsize 的影响

   2.常用在RNN网络，但如果输入的特征区别很大，那么就不建议使用它做归一化处理

	Instance Normalization

   1.IN的计算就是把每个HW单独拿出来归一化处理，不受通道和batchsize 的影响

   2.常用在风格化迁移，但如果特征图可以用到通道之间的相关性，那么就不建议使用它做归一化处理
### 23、KNN算法中，K值大小的影响
K值越大，偏差越大、方差越小，容易欠拟合

### 24、Adam算法的偏差校正
除了每一时刻平方梯度衰减的加权平均值（如Adadelta和RMSprop）之外，Adam还存储每一时刻梯度衰减指数mt的加权平均值。使用以下公式计算mt和vt：

image.png

mt和vt分别是梯度中第一时刻（平均值）和第二时刻（未中心化方差）的估计值，在初始化时衰减率很小（即β1和β2接近1），mt和vt被初始化为零向量。

Adam算法的设计者利用偏差校正第一时刻和第二时刻的估计值来抵消这些偏差，更新公式如下：

 image.png
 
 ### 25、one-hot编码与label-hot的区别
   对于定类类型的数据，建议使用one-hot encoding，对于定序类型的数据，建议使用label encoding。 定序类型也是分类，但有排序逻辑关系，等级上高于定类。比如，学历分小学，初中，高中，本科，研究生，各个类别之间存在一定的逻辑。
   对数值大小敏感的模型必须使用one-hotencoding。 典型的例子就是LR和SVM。二者的损失函数对数值大小是敏感的。对数值大小不敏感的模型（如树模型）不建议使用one-hotencoding。 一般这类模型为树模型。如果分类类别特别多，那么one-hot encoding会分裂出很多特征变量。这时候，如果我们限制了树模型的深度而不能向下分裂的话，一些特征变量可能就因为模型无法继续分裂而被舍弃损失掉了。因此，此种情况下可以考虑使用Label encoding。
# 刷题复习

​		**3月5日**：121.买卖股票、5.最长回文子串（难点）、1143.最长公共子序列、91解码方法(重点)、139、1143

​		**3月6日**：72字符串编辑距离（难）、650.、10（放弃）、122（股票）、123、213、53.最大子序和、343.整数拆分、583两个字符串的删除距离

​		**3月7日**：376.摆动序列、

​		**3月8日**：206、215、3无重复最长子串、25.K个一组翻转链表、

​		**3月9日**：142.环形链表、

重刷33、887、224、42、264、43、54、227、31、394、约瑟夫环、378

// 由层序序列和中序序列建立二叉树



栈中最小元素155

拓扑排序

滑动窗口最大值239

最长有效括号32

[二叉树](https://www.nowcoder.com/jump/super-jump/word?word=二叉树)的序列化与反序列化。297

一个环，有n个点, 问从0点出发，经过k步回到原点有多少种方法

无序数组求大小相邻两个数之间的最大差

待复习：全排列系列（时间复杂度分析）、AUC代码实现

面试时遇到的一些[机器学习]()编程题有：用python写数据的标准化预处理，用python写logistic regression的训练过程，用python写batch normalization的前向传播过程，用python写adam，用python写auc的计算过程

LRU、LFU实现

构建哈夫曼树、泰勒公式，用泰勒公式实现e的计算求值

315线段树

84、85最大矩形

[43. 1～n整数中1出现的次数]

【LeetCode 】416. 分割等和子集（使得两个子集的元素和相等）

【LeetCode 】494. 目标和 ( + 和 - 操作得到 target)]

剑指 Offer 11. 旋转数组的最小数字
